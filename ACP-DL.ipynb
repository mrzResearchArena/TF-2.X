{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ACP-DL.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPHaXxV7JCNfRFNJc7Zzy6k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mrzResearchArena/TF-2.X/blob/master/ACP-DL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFBQJdwMXsGJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "outputId": "d23001e4-4453-4850-f6c1-fae54754bb35"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXz4tvViYCqR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6e167696-32ad-4e8b-fc99-1f08560e9d12"
      },
      "source": [
        "ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mdrive\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7br7sG-YDBk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "861e31b9-a331-4a3d-94ea-2ccf55f6d13b"
      },
      "source": [
        "cd 'drive/My Drive/Colab-Notebooks'"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab-Notebooks\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jiXxsInoYKls",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "outputId": "95352cd4-590a-43b7-f99b-8204634f5ef1"
      },
      "source": [
        "ls -1"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0_FINAL_anti_peptide_model_AC240-tensorboard.ipynb\n",
            "0_FINAL_anti_peptide_model_ACP740-tensorboard.ipynb\n",
            "544_encoded_sequences_740.npy\n",
            "ACP240_labels.npy\n",
            "acp240.txt\n",
            "ACP740_labels.npy\n",
            "ACP-740-tensorboard.ipynb\n",
            "acp740.txt\n",
            "blosum62-acp240.npy\n",
            "blosum62-acp740.npy\n",
            "BPF_coded_ACP240_sequences.npy\n",
            "BPF_coded_ACP740_sequences.npy\n",
            "kmer_k_3_ACP240.npy\n",
            "kmer_k_3_ACP740.npy\n",
            "\u001b[0m\u001b[01;34mlogs\u001b[0m/\n",
            "mACP-240.ipynb\n",
            "mACP-740.ipynb\n",
            "model-240.png\n",
            "model-740.png\n",
            "model.png\n",
            "multichannel.png\n",
            "physico_coded_ACP240_sequences.npy\n",
            "physico_coded_ACP740_sequences.npy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHh09gasYMKl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, Dropout, Activation, GRU, SimpleRNN\n",
        "from keras.callbacks import ModelCheckpoint,TensorBoard\n",
        "from keras.optimizers import RMSprop\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.utils import plot_model\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pickle\n",
        "import scipy.io as sio\n",
        "import numpy as np\n",
        "from numpy import linalg as la\n",
        "import argparse"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZB4LwgYXeGO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def TransDict_from_list(groups):\n",
        "    transDict = dict()\n",
        "    tar_list = ['0', '1', '2', '3', '4', '5', '6']\n",
        "    result = {}\n",
        "    index = 0\n",
        "    for group in groups:\n",
        "        g_members = sorted(group)  # Alphabetically sorted list\n",
        "        for c in g_members:\n",
        "            # print('c' + str(c))\n",
        "            # print('g_members[0]' + str(g_members[0]))\n",
        "            result[c] = str(tar_list[index])  # K:V map, use group's first letter as represent.\n",
        "        index = index + 1\n",
        "    return result\n",
        "\n",
        "def get_3_protein_trids():\n",
        "    nucle_com = []\n",
        "    chars = ['0', '1', '2', '3', '4', '5', '6']\n",
        "    base = len(chars)\n",
        "    end = len(chars) ** 3\n",
        "    for i in range(0, end):\n",
        "        n = i\n",
        "        ch0 = chars[n % base]\n",
        "        n = n / base\n",
        "        ch1 = chars[int(n % base)]\n",
        "        n = n / base\n",
        "        ch2 = chars[int(n % base)]\n",
        "        nucle_com.append(ch0 + ch1 + ch2)\n",
        "    return nucle_com\n",
        "\n",
        "def translate_sequence(seq, TranslationDict):\n",
        "    '''\n",
        "    Given (seq) - a string/sequence to translate,\n",
        "    Translates into a reduced alphabet, using a translation dict provided\n",
        "    by the TransDict_from_list() method.\n",
        "    Returns the string/sequence in the new, reduced alphabet.\n",
        "    Remember - in Python string are immutable..\n",
        "\n",
        "    '''\n",
        "    import string\n",
        "    from_list = []\n",
        "    to_list = []\n",
        "    for k, v in TranslationDict.items():\n",
        "        from_list.append(k)\n",
        "        to_list.append(v)\n",
        "    # TRANS_seq = seq.translate(str.maketrans(zip(from_list,to_list)))\n",
        "    TRANS_seq = seq.translate(str.maketrans(str(from_list), str(to_list)))\n",
        "    # TRANS_seq = maketrans( TranslationDict, seq)\n",
        "    return TRANS_seq\n",
        "\n",
        "def get_4_nucleotide_composition(tris, seq, pythoncount=True):\n",
        "    seq_len = len(seq)\n",
        "    tri_feature = [0] * len(tris)\n",
        "    k = len(tris[0])\n",
        "    note_feature = [[0 for cols in range(len(seq) - k + 1)] for rows in range(len(tris))]\n",
        "    if pythoncount:\n",
        "        for val in tris:\n",
        "            num = seq.count(val)\n",
        "            tri_feature.append(float(num) / seq_len)\n",
        "    else:\n",
        "        # tmp_fea = [0] * len(tris)\n",
        "        for x in range(len(seq) + 1 - k):\n",
        "            kmer = seq[x:x + k]\n",
        "            if kmer in tris:\n",
        "                ind = tris.index(kmer)\n",
        "                # tmp_fea[ind] = tmp_fea[ind] + 1\n",
        "                note_feature[ind][x] = note_feature[ind][x] + 1\n",
        "        # tri_feature = [float(val)/seq_len for val in tmp_fea]    #tri_feature type:list len:256\n",
        "        u, s, v = la.svd(note_feature)\n",
        "        for i in range(len(s)):\n",
        "            tri_feature = tri_feature + u[i] * s[i] / seq_len\n",
        "        # print tri_feature\n",
        "        # pdb.set_trace()\n",
        "\n",
        "    return tri_feature\n",
        "# def get_4_nucleotide_composition(tris, seq, pythoncount=True):\n",
        "#     seq_len = len(seq)\n",
        "#     tri_feature = []\n",
        "#\n",
        "#     if pythoncount:\n",
        "#         for val in tris:\n",
        "#             num = seq.count(val)\n",
        "#             tri_feature.append(float(num) / seq_len)\n",
        "#     else:\n",
        "#         k = len(tris[0])\n",
        "#         tmp_fea = [0] * len(tris)\n",
        "#         for x in range(len(seq) + 1 - k):\n",
        "#             kmer = seq[x:x + k]\n",
        "#             if kmer in tris:\n",
        "#                 ind = tris.index(kmer)\n",
        "#                 tmp_fea[ind] = tmp_fea[ind] + 1\n",
        "#         tri_feature = [float(val) / seq_len for val in tmp_fea]\n",
        "#         # pdb.set_trace()\n",
        "#     return tri_feature\n",
        "\n",
        "def prepare_feature_acp740():\n",
        "    label = []\n",
        "    interaction_pair = {}\n",
        "    RNA_seq_dict = {}\n",
        "    protein_seq_dict = {}\n",
        "    protein_index = 0\n",
        "    with open('acp740.txt', 'r') as fp:\n",
        "        for line in fp:\n",
        "            if line[0] == '>':\n",
        "                values = line[1:].strip().split('|')\n",
        "                label_temp = values[1]\n",
        "                proteinName = values[0]\n",
        "                if label_temp == '1':\n",
        "                    label.append(1)\n",
        "                else:\n",
        "                    label.append(0)\n",
        "            else:\n",
        "                seq = line[:-1]\n",
        "                protein_seq_dict[protein_index] = seq\n",
        "                protein_index = protein_index + 1\n",
        "    # name_list = read_name_from_lncRNA_fasta('ncRNA-protein/lncRNA_RNA.fa')\n",
        "    groups = ['AGV', 'ILFP', 'YMTS', 'HNQW', 'RK', 'DE', 'C']\n",
        "    group_dict = TransDict_from_list(groups)\n",
        "    protein_tris = get_3_protein_trids()\n",
        "    # tris3 = get_3_trids()\n",
        "    bpf=[]\n",
        "    kmer=[]\n",
        "    # get protein feature\n",
        "    # pdb.set_trace()\n",
        "    for i in protein_seq_dict:  # and protein_fea_dict.has_key(protein) and RNA_fea_dict.has_key(RNA):\n",
        "        protein_seq = translate_sequence(protein_seq_dict[i], group_dict)\n",
        "        bpf_feature = BPF(protein_seq_dict[i])\n",
        "        # print('bpf:',shape(bpf_feature))\n",
        "        # pdb.set_trace()\n",
        "        # RNA_tri_fea = get_4_nucleotide_composition(tris, RNA_seq, pythoncount=False)\n",
        "        protein_tri_fea = get_4_nucleotide_composition(protein_tris, protein_seq, pythoncount =False)\n",
        "\n",
        "        bpf.append(bpf_feature)\n",
        "        kmer.append(protein_tri_fea)\n",
        "        # protein_index = protein_index + 1\n",
        "        # chem_fea.append(chem_tmp_fea)\n",
        "    return np.array(bpf), np.array(kmer), label\n",
        "\n",
        "def prepare_feature_acp240():\n",
        "    label = []\n",
        "    interaction_pair = {}\n",
        "    RNA_seq_dict = {}\n",
        "    protein_seq_dict = {}\n",
        "    protein_index = 1\n",
        "    with open('acp240.txt', 'r') as fp:\n",
        "        for line in fp:\n",
        "            if line[0] == '>':\n",
        "                values = line[1:].strip().split('|')\n",
        "                label_temp = values[1]\n",
        "                protein = values[0]\n",
        "                if label_temp=='1':\n",
        "                    label.append(1)\n",
        "                else:\n",
        "                    label.append(0)\n",
        "            else:\n",
        "                seq = line[:-1]\n",
        "                protein_seq_dict[protein_index] = seq\n",
        "                protein_index = protein_index + 1\n",
        "    # name_list = read_name_from_lncRNA_fasta('ncRNA-protein/lncRNA_RNA.fa')\n",
        "    groups = ['AGV', 'ILFP', 'YMTS', 'HNQW', 'RK', 'DE', 'C']\n",
        "    group_dict = TransDict_from_list(groups)\n",
        "    protein_tris = get_3_protein_trids()\n",
        "    # tris3 = get_3_trids()\n",
        "    bpf = []\n",
        "    kmer = []\n",
        "    # get protein feature\n",
        "    # pdb.set_trace()\n",
        "    for i in protein_seq_dict:  # and protein_fea_dict.has_key(protein) and RNA_fea_dict.has_key(RNA):\n",
        "\n",
        "        protein_seq = translate_sequence(protein_seq_dict[i], group_dict)\n",
        "        bpf_feature = BPF(protein_seq_dict[i])\n",
        "        # print('bpf:',shape(bpf_feature))\n",
        "        # pdb.set_trace()\n",
        "        # RNA_tri_fea = get_4_nucleotide_composition(tris, RNA_seq, pythoncount=False)\n",
        "        protein_tri_fea = get_4_nucleotide_composition(protein_tris, protein_seq, pythoncount =False)\n",
        "\n",
        "        bpf.append(bpf_feature)\n",
        "        kmer.append(protein_tri_fea)\n",
        "        protein_index = protein_index + 1\n",
        "        # chem_fea.append(chem_tmp_fea)\n",
        "\n",
        "    return np.array(bpf), np.array(kmer), label\n",
        "\n",
        "def BPF(seq_temp):\n",
        "    seq = seq_temp\n",
        "    chars = ['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y']\n",
        "    fea = []\n",
        "    tem_vec =[]\n",
        "    k = 7\n",
        "    for i in range(k):\n",
        "        if seq[i] =='A':\n",
        "            tem_vec = [1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
        "        elif seq[i]=='C':\n",
        "            tem_vec = [0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
        "        elif seq[i]=='D':\n",
        "            tem_vec = [0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
        "        elif seq[i]=='E':\n",
        "            tem_vec = [0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
        "        elif seq[i]=='F':\n",
        "            tem_vec = [0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
        "        elif seq[i]=='G':\n",
        "            tem_vec = [0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
        "        elif seq[i]=='H':\n",
        "            tem_vec = [0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
        "        elif seq[i]=='I':\n",
        "            tem_vec = [0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0]\n",
        "        elif seq[i]=='K':\n",
        "            tem_vec = [0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0]\n",
        "        elif seq[i]=='L':\n",
        "            tem_vec = [0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0]\n",
        "        elif seq[i]=='M':\n",
        "            tem_vec = [0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0]\n",
        "        elif seq[i]=='N':\n",
        "            tem_vec = [0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0]\n",
        "        elif seq[i]=='P':\n",
        "            tem_vec = [0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0]\n",
        "        elif seq[i]=='Q':\n",
        "            tem_vec = [0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0]\n",
        "        elif seq[i]=='R':\n",
        "            tem_vec = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0]\n",
        "        elif seq[i]=='S':\n",
        "            tem_vec = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0]\n",
        "        elif seq[i]=='T':\n",
        "            tem_vec = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0]\n",
        "        elif seq[i]=='V':\n",
        "            tem_vec = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0]\n",
        "        elif seq[i]=='W':\n",
        "            tem_vec = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0]\n",
        "        elif seq[i]=='Y':\n",
        "            tem_vec = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1]\n",
        "        fea = fea + tem_vec\n",
        "    return fea\n",
        "\n",
        "def prepare_feature():\n",
        "    label = []\n",
        "    interaction_pair = {}\n",
        "    RNA_seq_dict = {}\n",
        "    protein_seq_dict = {}\n",
        "    protein_index = 1\n",
        "    with open('acp740.txt', 'r') as fp:\n",
        "        for line in fp:\n",
        "            if line[0] == '>':\n",
        "                values = line[1:].strip().split('|')\n",
        "                label_temp = values[1]\n",
        "                protein = values[0]\n",
        "                if label_temp == '1':\n",
        "                    label.append(1)\n",
        "                else:\n",
        "                    label.append(0)\n",
        "            else:\n",
        "                seq = line[:-1]\n",
        "                protein_seq_dict[protein_index] = seq\n",
        "                protein_index = protein_index + 1\n",
        "    # name_list = read_name_from_lncRNA_fasta('ncRNA-protein/lncRNA_RNA.fa')\n",
        "    groups = ['AGV', 'ILFP', 'YMTS', 'HNQW', 'RK', 'DE', 'C']\n",
        "    group_dict = TransDict_from_list(groups)\n",
        "    protein_tris = get_3_protein_trids()\n",
        "\n",
        "    # tris3 = get_3_trids()\n",
        "    train = []\n",
        "    # get protein feature\n",
        "    # pdb.set_trace()\n",
        "    for i in protein_seq_dict:  # and protein_fea_dict.has_key(protein) and RNA_fea_dict.has_key(RNA):\n",
        "\n",
        "        protein_seq = translate_sequence(protein_seq_dict[i], group_dict)\n",
        "        # bpf_feature = BPF(protein_seq_dict[i])\n",
        "        # pdb.set_trace()\n",
        "        # RNA_tri_fea = get_4_nucleotide_composition(tris, RNA_seq, pythoncount=False)\n",
        "        protein_tri_fea = get_4_nucleotide_composition(protein_tris, protein_seq, pythoncount =False)\n",
        "\n",
        "        train.append(protein_tri_fea)\n",
        "        protein_index = protein_index + 1\n",
        "        # chem_fea.append(chem_tmp_fea)\n",
        "\n",
        "    return np.array(train), label\n",
        "def calculate_performace(test_num, pred_y, labels):\n",
        "    tp = 0\n",
        "    fp = 0\n",
        "    tn = 0\n",
        "    fn = 0\n",
        "    for index in range(test_num):\n",
        "        if labels[index] == 1:\n",
        "            if labels[index] == pred_y[index]:\n",
        "                tp = tp + 1\n",
        "            else:\n",
        "                fn = fn + 1\n",
        "        else:\n",
        "            if labels[index] == pred_y[index]:\n",
        "                tn = tn + 1\n",
        "            else:\n",
        "                fp = fp + 1\n",
        "\n",
        "    acc = float(tp + tn) / test_num\n",
        "    precision = float(tp) / (tp + fp)\n",
        "    sensitivity = float(tp) / (tp + fn)\n",
        "    specificity = float(tn) / (tn + fp)\n",
        "    MCC = float(tp * tn - fp * fn) / (np.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)))\n",
        "    return acc, precision, sensitivity, specificity, MCC\n",
        "\n",
        "def transfer_label_from_prob(proba):\n",
        "    label = [1 if val >= 0.5 else 0 for val in proba]\n",
        "    return label\n",
        "def plot_roc_curve(labels, probality, legend_text, auc_tag=True):\n",
        "    # fpr2, tpr2, thresholds = roc_curve(labels, pred_y)\n",
        "    fpr, tpr, thresholds = roc_curve(labels, probality)  # probas_[:, 1])\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    if auc_tag:\n",
        "        rects1 = plt.plot(fpr, tpr, label=legend_text + ' (AUC=%6.3f) ' % roc_auc)\n",
        "    else:\n",
        "        rects1 = plt.plot(fpr, tpr, label=legend_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFdZ28RjX7ak",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "79b0ba90-f539-4651-a7b3-857214dc1040"
      },
      "source": [
        "def ACP_DL():\n",
        "    # define parameters\n",
        "    data_dim = 483\n",
        "    timesteps = 1\n",
        "    batch_size = 32   # if dataset = acp240, set batch_size = 32; if dataset = acp740, set batch_size = 64\n",
        "    epochs = 100\n",
        "    # get data\n",
        "\n",
        "    # bpf, kmer, label = prepare_feature_acp740()\n",
        "    bpf, kmer, label = prepare_feature_acp240()\n",
        "    X = np.concatenate((bpf, kmer), axis=1)  # 1 行拼接 0 默认，列拼接\n",
        "    #  expected input data shape: (batch_size, timesteps, data_dim)\n",
        "    X = np.reshape(X, (len(X), timesteps, data_dim))\n",
        "    # split data\n",
        "    # x_train, x_test, y_train, y_test = train_test_split(X, label, test_size=0.1, random_state=1024)\n",
        "    num_cross_val = 5  # 5-fold\n",
        "    all_performance_lstm = []\n",
        "    all_labels = []\n",
        "    all_prob = {}\n",
        "    num_classifier = 3\n",
        "    all_prob[0] = []\n",
        "    all_average = []\n",
        "\n",
        "    for fold in range(num_cross_val):\n",
        "        # train = np.array([x for i, x in enumerate(bpf_fea) if i % num_cross_val != fold])\n",
        "        # test = np.array([x for i, x in enumerate(bpf_fea) if i % num_cross_val == fold])\n",
        "        # train = np.array([x for i, x in enumerate(kmer_fea) if i % num_cross_val != fold])\n",
        "        # test = np.array([x for i, x in enumerate(kmer_fea) if i % num_cross_val == fold])\n",
        "        train = np.array([x for i, x in enumerate(X) if i % num_cross_val != fold])\n",
        "        test = np.array([x for i, x in enumerate(X) if i % num_cross_val == fold])\n",
        "        train_label = np.array([x for i, x in enumerate(label) if i % num_cross_val != fold])\n",
        "        test_label = np.array([x for i, x in enumerate(label) if i % num_cross_val == fold])\n",
        "        real_labels = []\n",
        "        for val in test_label:\n",
        "            if val == 1:\n",
        "                real_labels.append(1)\n",
        "            else:\n",
        "                real_labels.append(0)\n",
        "\n",
        "        train_label_new = []\n",
        "        for val in train_label:\n",
        "            if val == 1:\n",
        "                train_label_new.append(1)\n",
        "            else:\n",
        "                train_label_new.append(0)\n",
        "        all_labels = all_labels + real_labels\n",
        "        # init = initializers.TruncatedNormal(mean=0.0, stddev=0.05, seed=64)\n",
        "\n",
        "        model = Sequential()\n",
        "        # model.add(Dense(200,input_shape=(200,1)))\n",
        "        model.add(LSTM(128, return_sequences=False,input_shape=(timesteps, data_dim), name='lstm1'))  # returns a sequence of vectors of dimension 32\n",
        "        #model.add(LSTM(32, return_sequences=True, name='lstm2'))  # returns a sequence of vectors of dimension 32\n",
        "        #model.add(LSTM(8, return_sequences=True,name='lstm3'))  # returns a sequence of vectors of dimension 32\n",
        "        #model.add(LSTM(3, return_sequences=False,name='lstm4'))  # return a single vector of dimension 32\n",
        "        model.add(Dropout(0.25, name='dropout'))\n",
        "        model.add(Dense(1, name='full_connect'))\n",
        "        model.add(Activation('sigmoid'))\n",
        "        model.summary()\n",
        "\n",
        "        print('Compiling the Model...')\n",
        "        model.compile(loss='binary_crossentropy',  #\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "        print(\"Train...\")\n",
        "\n",
        "        model.fit(train, train_label, batch_size=batch_size,epochs=epochs)\n",
        "\n",
        "        lstm_proba = model.predict_proba(test)\n",
        "        all_prob[0] = all_prob[0] + [val for val in lstm_proba]\n",
        "        y_pred_xgb = transfer_label_from_prob(lstm_proba)\n",
        "        acc, precision, sensitivity, specificity, MCC = calculate_performace(len(real_labels), y_pred_xgb, real_labels)\n",
        "        print(acc, precision, sensitivity, specificity, MCC)\n",
        "        all_performance_lstm.append([acc, precision, sensitivity, specificity, MCC])\n",
        "        print('---' * 50)\n",
        "\n",
        "    print('mean performance of ACP_DL')\n",
        "    print(np.mean(np.array(all_performance_lstm), axis=0))\n",
        "    print('---' * 50)\n",
        "\n",
        "    plot_roc_curve(all_labels, all_prob[0], 'proposed method')\n",
        "    plt.plot([0, 1], [0, 1], 'k--')\n",
        "    plt.xlim([-0.05, 1])\n",
        "    plt.ylim([0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('ROC')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    # plt.savefig(save_fig_dir + selected + '_' + class_type + '.png')\n",
        "    plt.show()\n",
        "ACP_DL()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_22\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm1 (LSTM)                 (None, 128)               313344    \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "full_connect (Dense)         (None, 1)                 129       \n",
            "_________________________________________________________________\n",
            "activation_22 (Activation)   (None, 1)                 0         \n",
            "=================================================================\n",
            "Total params: 313,473\n",
            "Trainable params: 313,473\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Compiling the Model...\n",
            "Train...\n",
            "Epoch 1/100\n",
            "192/192 [==============================] - 5s 24ms/step - loss: 0.6909 - acc: 0.5417\n",
            "Epoch 2/100\n",
            "192/192 [==============================] - 0s 434us/step - loss: 0.6714 - acc: 0.7448\n",
            "Epoch 3/100\n",
            "192/192 [==============================] - 0s 430us/step - loss: 0.6541 - acc: 0.8281\n",
            "Epoch 4/100\n",
            "192/192 [==============================] - 0s 432us/step - loss: 0.6383 - acc: 0.8281\n",
            "Epoch 5/100\n",
            "192/192 [==============================] - 0s 434us/step - loss: 0.6207 - acc: 0.8646\n",
            "Epoch 6/100\n",
            "192/192 [==============================] - 0s 370us/step - loss: 0.5981 - acc: 0.8750\n",
            "Epoch 7/100\n",
            "192/192 [==============================] - 0s 421us/step - loss: 0.5783 - acc: 0.8594\n",
            "Epoch 8/100\n",
            "192/192 [==============================] - 0s 375us/step - loss: 0.5568 - acc: 0.8750\n",
            "Epoch 9/100\n",
            "192/192 [==============================] - 0s 421us/step - loss: 0.5334 - acc: 0.8802\n",
            "Epoch 10/100\n",
            "192/192 [==============================] - 0s 391us/step - loss: 0.5085 - acc: 0.8906\n",
            "Epoch 11/100\n",
            "192/192 [==============================] - 0s 386us/step - loss: 0.4835 - acc: 0.8854\n",
            "Epoch 12/100\n",
            "192/192 [==============================] - 0s 436us/step - loss: 0.4562 - acc: 0.8854\n",
            "Epoch 13/100\n",
            "192/192 [==============================] - 0s 431us/step - loss: 0.4280 - acc: 0.8906\n",
            "Epoch 14/100\n",
            "192/192 [==============================] - 0s 390us/step - loss: 0.4082 - acc: 0.8906\n",
            "Epoch 15/100\n",
            "192/192 [==============================] - 0s 426us/step - loss: 0.3761 - acc: 0.8958\n",
            "Epoch 16/100\n",
            "192/192 [==============================] - 0s 389us/step - loss: 0.3555 - acc: 0.8906\n",
            "Epoch 17/100\n",
            "192/192 [==============================] - 0s 394us/step - loss: 0.3318 - acc: 0.8958\n",
            "Epoch 18/100\n",
            "192/192 [==============================] - 0s 455us/step - loss: 0.3118 - acc: 0.9115\n",
            "Epoch 19/100\n",
            "192/192 [==============================] - 0s 422us/step - loss: 0.2904 - acc: 0.9167\n",
            "Epoch 20/100\n",
            "192/192 [==============================] - 0s 440us/step - loss: 0.2702 - acc: 0.9219\n",
            "Epoch 21/100\n",
            "192/192 [==============================] - 0s 397us/step - loss: 0.2545 - acc: 0.9375\n",
            "Epoch 22/100\n",
            "192/192 [==============================] - 0s 482us/step - loss: 0.2357 - acc: 0.9427\n",
            "Epoch 23/100\n",
            "192/192 [==============================] - 0s 499us/step - loss: 0.2197 - acc: 0.9427\n",
            "Epoch 24/100\n",
            "192/192 [==============================] - 0s 472us/step - loss: 0.2039 - acc: 0.9583\n",
            "Epoch 25/100\n",
            "192/192 [==============================] - 0s 511us/step - loss: 0.1900 - acc: 0.9583\n",
            "Epoch 26/100\n",
            "192/192 [==============================] - 0s 442us/step - loss: 0.1770 - acc: 0.9688\n",
            "Epoch 27/100\n",
            "192/192 [==============================] - 0s 431us/step - loss: 0.1600 - acc: 0.9792\n",
            "Epoch 28/100\n",
            "192/192 [==============================] - 0s 440us/step - loss: 0.1529 - acc: 0.9688\n",
            "Epoch 29/100\n",
            "192/192 [==============================] - 0s 460us/step - loss: 0.1420 - acc: 0.9792\n",
            "Epoch 30/100\n",
            "192/192 [==============================] - 0s 461us/step - loss: 0.1320 - acc: 0.9844\n",
            "Epoch 31/100\n",
            "192/192 [==============================] - 0s 456us/step - loss: 0.1247 - acc: 0.9792\n",
            "Epoch 32/100\n",
            "192/192 [==============================] - 0s 443us/step - loss: 0.1158 - acc: 0.9896\n",
            "Epoch 33/100\n",
            "192/192 [==============================] - 0s 462us/step - loss: 0.1085 - acc: 0.9896\n",
            "Epoch 34/100\n",
            "192/192 [==============================] - 0s 470us/step - loss: 0.0979 - acc: 0.9896\n",
            "Epoch 35/100\n",
            "192/192 [==============================] - 0s 484us/step - loss: 0.0918 - acc: 0.9896\n",
            "Epoch 36/100\n",
            "192/192 [==============================] - 0s 452us/step - loss: 0.0886 - acc: 0.9896\n",
            "Epoch 37/100\n",
            "192/192 [==============================] - 0s 478us/step - loss: 0.0813 - acc: 0.9896\n",
            "Epoch 38/100\n",
            "192/192 [==============================] - 0s 397us/step - loss: 0.0759 - acc: 0.9896\n",
            "Epoch 39/100\n",
            "192/192 [==============================] - 0s 423us/step - loss: 0.0709 - acc: 0.9896\n",
            "Epoch 40/100\n",
            "192/192 [==============================] - 0s 430us/step - loss: 0.0667 - acc: 0.9896\n",
            "Epoch 41/100\n",
            "192/192 [==============================] - 0s 376us/step - loss: 0.0641 - acc: 0.9896\n",
            "Epoch 42/100\n",
            "192/192 [==============================] - 0s 383us/step - loss: 0.0593 - acc: 0.9948\n",
            "Epoch 43/100\n",
            "192/192 [==============================] - 0s 390us/step - loss: 0.0556 - acc: 0.9948\n",
            "Epoch 44/100\n",
            "192/192 [==============================] - 0s 455us/step - loss: 0.0526 - acc: 0.9948\n",
            "Epoch 45/100\n",
            "192/192 [==============================] - 0s 430us/step - loss: 0.0477 - acc: 0.9948\n",
            "Epoch 46/100\n",
            "192/192 [==============================] - 0s 461us/step - loss: 0.0463 - acc: 0.9948\n",
            "Epoch 47/100\n",
            "192/192 [==============================] - 0s 467us/step - loss: 0.0430 - acc: 0.9948\n",
            "Epoch 48/100\n",
            "192/192 [==============================] - 0s 453us/step - loss: 0.0408 - acc: 0.9948\n",
            "Epoch 49/100\n",
            "192/192 [==============================] - 0s 462us/step - loss: 0.0375 - acc: 0.9948\n",
            "Epoch 50/100\n",
            "192/192 [==============================] - 0s 480us/step - loss: 0.0377 - acc: 0.9948\n",
            "Epoch 51/100\n",
            "192/192 [==============================] - 0s 453us/step - loss: 0.0337 - acc: 0.9948\n",
            "Epoch 52/100\n",
            "192/192 [==============================] - 0s 462us/step - loss: 0.0309 - acc: 0.9948\n",
            "Epoch 53/100\n",
            "192/192 [==============================] - 0s 632us/step - loss: 0.0300 - acc: 0.9948\n",
            "Epoch 54/100\n",
            "192/192 [==============================] - 0s 474us/step - loss: 0.0293 - acc: 0.9948\n",
            "Epoch 55/100\n",
            "192/192 [==============================] - 0s 461us/step - loss: 0.0269 - acc: 0.9948\n",
            "Epoch 56/100\n",
            "192/192 [==============================] - 0s 416us/step - loss: 0.0264 - acc: 0.9948\n",
            "Epoch 57/100\n",
            "192/192 [==============================] - 0s 413us/step - loss: 0.0252 - acc: 0.9948\n",
            "Epoch 58/100\n",
            "192/192 [==============================] - 0s 418us/step - loss: 0.0227 - acc: 0.9948\n",
            "Epoch 59/100\n",
            "192/192 [==============================] - 0s 452us/step - loss: 0.0227 - acc: 0.9948\n",
            "Epoch 60/100\n",
            "192/192 [==============================] - 0s 482us/step - loss: 0.0221 - acc: 1.0000\n",
            "Epoch 61/100\n",
            "192/192 [==============================] - 0s 488us/step - loss: 0.0192 - acc: 1.0000\n",
            "Epoch 62/100\n",
            "192/192 [==============================] - 0s 451us/step - loss: 0.0174 - acc: 1.0000\n",
            "Epoch 63/100\n",
            "192/192 [==============================] - 0s 490us/step - loss: 0.0163 - acc: 1.0000\n",
            "Epoch 64/100\n",
            "192/192 [==============================] - 0s 503us/step - loss: 0.0170 - acc: 1.0000\n",
            "Epoch 65/100\n",
            "192/192 [==============================] - 0s 440us/step - loss: 0.0163 - acc: 1.0000\n",
            "Epoch 66/100\n",
            "192/192 [==============================] - 0s 395us/step - loss: 0.0166 - acc: 1.0000\n",
            "Epoch 67/100\n",
            "192/192 [==============================] - 0s 380us/step - loss: 0.0150 - acc: 1.0000\n",
            "Epoch 68/100\n",
            "192/192 [==============================] - 0s 367us/step - loss: 0.0146 - acc: 1.0000\n",
            "Epoch 69/100\n",
            "192/192 [==============================] - 0s 442us/step - loss: 0.0134 - acc: 1.0000\n",
            "Epoch 70/100\n",
            "192/192 [==============================] - 0s 392us/step - loss: 0.0117 - acc: 1.0000\n",
            "Epoch 71/100\n",
            "192/192 [==============================] - 0s 400us/step - loss: 0.0121 - acc: 1.0000\n",
            "Epoch 72/100\n",
            "192/192 [==============================] - 0s 375us/step - loss: 0.0112 - acc: 1.0000\n",
            "Epoch 73/100\n",
            "192/192 [==============================] - 0s 412us/step - loss: 0.0105 - acc: 1.0000\n",
            "Epoch 74/100\n",
            "192/192 [==============================] - 0s 380us/step - loss: 0.0102 - acc: 1.0000\n",
            "Epoch 75/100\n",
            "192/192 [==============================] - 0s 448us/step - loss: 0.0097 - acc: 1.0000\n",
            "Epoch 76/100\n",
            "192/192 [==============================] - 0s 455us/step - loss: 0.0099 - acc: 1.0000\n",
            "Epoch 77/100\n",
            "192/192 [==============================] - 0s 455us/step - loss: 0.0087 - acc: 1.0000\n",
            "Epoch 78/100\n",
            "192/192 [==============================] - 0s 488us/step - loss: 0.0094 - acc: 1.0000\n",
            "Epoch 79/100\n",
            "192/192 [==============================] - 0s 455us/step - loss: 0.0088 - acc: 1.0000\n",
            "Epoch 80/100\n",
            "192/192 [==============================] - 0s 449us/step - loss: 0.0085 - acc: 1.0000\n",
            "Epoch 81/100\n",
            "192/192 [==============================] - 0s 457us/step - loss: 0.0082 - acc: 1.0000\n",
            "Epoch 82/100\n",
            "192/192 [==============================] - 0s 475us/step - loss: 0.0076 - acc: 1.0000\n",
            "Epoch 83/100\n",
            "192/192 [==============================] - 0s 444us/step - loss: 0.0073 - acc: 1.0000\n",
            "Epoch 84/100\n",
            "192/192 [==============================] - 0s 486us/step - loss: 0.0076 - acc: 1.0000\n",
            "Epoch 85/100\n",
            "192/192 [==============================] - 0s 419us/step - loss: 0.0073 - acc: 1.0000\n",
            "Epoch 86/100\n",
            "192/192 [==============================] - 0s 507us/step - loss: 0.0070 - acc: 1.0000\n",
            "Epoch 87/100\n",
            "192/192 [==============================] - 0s 457us/step - loss: 0.0062 - acc: 1.0000\n",
            "Epoch 88/100\n",
            "192/192 [==============================] - 0s 463us/step - loss: 0.0061 - acc: 1.0000\n",
            "Epoch 89/100\n",
            "192/192 [==============================] - 0s 435us/step - loss: 0.0068 - acc: 1.0000\n",
            "Epoch 90/100\n",
            "192/192 [==============================] - 0s 476us/step - loss: 0.0066 - acc: 1.0000\n",
            "Epoch 91/100\n",
            "192/192 [==============================] - 0s 441us/step - loss: 0.0057 - acc: 1.0000\n",
            "Epoch 92/100\n",
            "192/192 [==============================] - 0s 495us/step - loss: 0.0059 - acc: 1.0000\n",
            "Epoch 93/100\n",
            "192/192 [==============================] - 0s 453us/step - loss: 0.0053 - acc: 1.0000\n",
            "Epoch 94/100\n",
            "192/192 [==============================] - 0s 448us/step - loss: 0.0054 - acc: 1.0000\n",
            "Epoch 95/100\n",
            "192/192 [==============================] - 0s 446us/step - loss: 0.0048 - acc: 1.0000\n",
            "Epoch 96/100\n",
            "192/192 [==============================] - 0s 452us/step - loss: 0.0052 - acc: 1.0000\n",
            "Epoch 97/100\n",
            "192/192 [==============================] - 0s 472us/step - loss: 0.0050 - acc: 1.0000\n",
            "Epoch 98/100\n",
            "192/192 [==============================] - 0s 471us/step - loss: 0.0047 - acc: 1.0000\n",
            "Epoch 99/100\n",
            "192/192 [==============================] - 0s 449us/step - loss: 0.0046 - acc: 1.0000\n",
            "Epoch 100/100\n",
            "192/192 [==============================] - 0s 480us/step - loss: 0.0048 - acc: 1.0000\n",
            "0.875 0.8846153846153846 0.8846153846153846 0.8636363636363636 0.7482517482517482\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Model: \"sequential_23\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm1 (LSTM)                 (None, 128)               313344    \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "full_connect (Dense)         (None, 1)                 129       \n",
            "_________________________________________________________________\n",
            "activation_23 (Activation)   (None, 1)                 0         \n",
            "=================================================================\n",
            "Total params: 313,473\n",
            "Trainable params: 313,473\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Compiling the Model...\n",
            "Train...\n",
            "Epoch 1/100\n",
            "192/192 [==============================] - 5s 25ms/step - loss: 0.6901 - acc: 0.5729\n",
            "Epoch 2/100\n",
            "192/192 [==============================] - 0s 383us/step - loss: 0.6737 - acc: 0.7396\n",
            "Epoch 3/100\n",
            "192/192 [==============================] - 0s 383us/step - loss: 0.6552 - acc: 0.8073\n",
            "Epoch 4/100\n",
            "192/192 [==============================] - 0s 386us/step - loss: 0.6363 - acc: 0.8333\n",
            "Epoch 5/100\n",
            "192/192 [==============================] - 0s 399us/step - loss: 0.6175 - acc: 0.8490\n",
            "Epoch 6/100\n",
            "192/192 [==============================] - 0s 394us/step - loss: 0.5977 - acc: 0.8750\n",
            "Epoch 7/100\n",
            "192/192 [==============================] - 0s 434us/step - loss: 0.5726 - acc: 0.8958\n",
            "Epoch 8/100\n",
            "192/192 [==============================] - 0s 432us/step - loss: 0.5472 - acc: 0.8906\n",
            "Epoch 9/100\n",
            "192/192 [==============================] - 0s 425us/step - loss: 0.5236 - acc: 0.8958\n",
            "Epoch 10/100\n",
            "192/192 [==============================] - 0s 458us/step - loss: 0.4972 - acc: 0.8906\n",
            "Epoch 11/100\n",
            "192/192 [==============================] - 0s 456us/step - loss: 0.4674 - acc: 0.8958\n",
            "Epoch 12/100\n",
            "192/192 [==============================] - 0s 461us/step - loss: 0.4360 - acc: 0.9167\n",
            "Epoch 13/100\n",
            "192/192 [==============================] - 0s 443us/step - loss: 0.4056 - acc: 0.9167\n",
            "Epoch 14/100\n",
            "192/192 [==============================] - 0s 488us/step - loss: 0.3830 - acc: 0.9219\n",
            "Epoch 15/100\n",
            "192/192 [==============================] - 0s 485us/step - loss: 0.3497 - acc: 0.9271\n",
            "Epoch 16/100\n",
            "192/192 [==============================] - 0s 470us/step - loss: 0.3258 - acc: 0.9271\n",
            "Epoch 17/100\n",
            "192/192 [==============================] - 0s 440us/step - loss: 0.3057 - acc: 0.9323\n",
            "Epoch 18/100\n",
            "192/192 [==============================] - 0s 449us/step - loss: 0.2794 - acc: 0.9323\n",
            "Epoch 19/100\n",
            "192/192 [==============================] - 0s 471us/step - loss: 0.2550 - acc: 0.9479\n",
            "Epoch 20/100\n",
            "192/192 [==============================] - 0s 463us/step - loss: 0.2424 - acc: 0.9531\n",
            "Epoch 21/100\n",
            "192/192 [==============================] - 0s 502us/step - loss: 0.2211 - acc: 0.9583\n",
            "Epoch 22/100\n",
            "192/192 [==============================] - 0s 452us/step - loss: 0.2050 - acc: 0.9635\n",
            "Epoch 23/100\n",
            "192/192 [==============================] - 0s 429us/step - loss: 0.1923 - acc: 0.9635\n",
            "Epoch 24/100\n",
            "192/192 [==============================] - 0s 442us/step - loss: 0.1789 - acc: 0.9688\n",
            "Epoch 25/100\n",
            "192/192 [==============================] - 0s 462us/step - loss: 0.1639 - acc: 0.9740\n",
            "Epoch 26/100\n",
            "192/192 [==============================] - 0s 493us/step - loss: 0.1532 - acc: 0.9688\n",
            "Epoch 27/100\n",
            "192/192 [==============================] - 0s 456us/step - loss: 0.1416 - acc: 0.9740\n",
            "Epoch 28/100\n",
            "192/192 [==============================] - 0s 368us/step - loss: 0.1360 - acc: 0.9740\n",
            "Epoch 29/100\n",
            "192/192 [==============================] - 0s 366us/step - loss: 0.1255 - acc: 0.9740\n",
            "Epoch 30/100\n",
            "192/192 [==============================] - 0s 371us/step - loss: 0.1169 - acc: 0.9740\n",
            "Epoch 31/100\n",
            "192/192 [==============================] - 0s 436us/step - loss: 0.1097 - acc: 0.9792\n",
            "Epoch 32/100\n",
            "192/192 [==============================] - 0s 416us/step - loss: 0.1005 - acc: 0.9844\n",
            "Epoch 33/100\n",
            "192/192 [==============================] - 0s 430us/step - loss: 0.0961 - acc: 0.9844\n",
            "Epoch 34/100\n",
            "192/192 [==============================] - 0s 424us/step - loss: 0.0902 - acc: 0.9896\n",
            "Epoch 35/100\n",
            "192/192 [==============================] - 0s 415us/step - loss: 0.0834 - acc: 0.9896\n",
            "Epoch 36/100\n",
            "192/192 [==============================] - 0s 431us/step - loss: 0.0799 - acc: 0.9896\n",
            "Epoch 37/100\n",
            "192/192 [==============================] - 0s 473us/step - loss: 0.0733 - acc: 0.9896\n",
            "Epoch 38/100\n",
            "192/192 [==============================] - 0s 450us/step - loss: 0.0683 - acc: 0.9948\n",
            "Epoch 39/100\n",
            "192/192 [==============================] - 0s 398us/step - loss: 0.0633 - acc: 0.9896\n",
            "Epoch 40/100\n",
            "192/192 [==============================] - 0s 355us/step - loss: 0.0606 - acc: 0.9896\n",
            "Epoch 41/100\n",
            "192/192 [==============================] - 0s 385us/step - loss: 0.0572 - acc: 0.9948\n",
            "Epoch 42/100\n",
            "192/192 [==============================] - 0s 365us/step - loss: 0.0529 - acc: 0.9948\n",
            "Epoch 43/100\n",
            "192/192 [==============================] - 0s 368us/step - loss: 0.0535 - acc: 0.9896\n",
            "Epoch 44/100\n",
            "192/192 [==============================] - 0s 462us/step - loss: 0.0477 - acc: 0.9948\n",
            "Epoch 45/100\n",
            "192/192 [==============================] - 0s 441us/step - loss: 0.0442 - acc: 0.9948\n",
            "Epoch 46/100\n",
            "192/192 [==============================] - 0s 492us/step - loss: 0.0428 - acc: 0.9948\n",
            "Epoch 47/100\n",
            "192/192 [==============================] - 0s 499us/step - loss: 0.0402 - acc: 0.9948\n",
            "Epoch 48/100\n",
            "192/192 [==============================] - 0s 491us/step - loss: 0.0376 - acc: 0.9948\n",
            "Epoch 49/100\n",
            "192/192 [==============================] - 0s 438us/step - loss: 0.0355 - acc: 1.0000\n",
            "Epoch 50/100\n",
            "192/192 [==============================] - 0s 430us/step - loss: 0.0327 - acc: 0.9948\n",
            "Epoch 51/100\n",
            "192/192 [==============================] - 0s 370us/step - loss: 0.0334 - acc: 1.0000\n",
            "Epoch 52/100\n",
            "192/192 [==============================] - 0s 392us/step - loss: 0.0291 - acc: 1.0000\n",
            "Epoch 53/100\n",
            "192/192 [==============================] - 0s 358us/step - loss: 0.0290 - acc: 1.0000\n",
            "Epoch 54/100\n",
            "192/192 [==============================] - 0s 391us/step - loss: 0.0243 - acc: 1.0000\n",
            "Epoch 55/100\n",
            "192/192 [==============================] - 0s 430us/step - loss: 0.0250 - acc: 1.0000\n",
            "Epoch 56/100\n",
            "192/192 [==============================] - 0s 451us/step - loss: 0.0233 - acc: 1.0000\n",
            "Epoch 57/100\n",
            "192/192 [==============================] - 0s 431us/step - loss: 0.0202 - acc: 1.0000\n",
            "Epoch 58/100\n",
            "192/192 [==============================] - 0s 488us/step - loss: 0.0223 - acc: 1.0000\n",
            "Epoch 59/100\n",
            "192/192 [==============================] - 0s 430us/step - loss: 0.0200 - acc: 1.0000\n",
            "Epoch 60/100\n",
            "192/192 [==============================] - 0s 402us/step - loss: 0.0191 - acc: 1.0000\n",
            "Epoch 61/100\n",
            "192/192 [==============================] - 0s 436us/step - loss: 0.0177 - acc: 1.0000\n",
            "Epoch 62/100\n",
            "192/192 [==============================] - 0s 494us/step - loss: 0.0172 - acc: 1.0000\n",
            "Epoch 63/100\n",
            "192/192 [==============================] - 0s 400us/step - loss: 0.0167 - acc: 1.0000\n",
            "Epoch 64/100\n",
            "192/192 [==============================] - 0s 432us/step - loss: 0.0150 - acc: 1.0000\n",
            "Epoch 65/100\n",
            "192/192 [==============================] - 0s 423us/step - loss: 0.0149 - acc: 1.0000\n",
            "Epoch 66/100\n",
            "192/192 [==============================] - 0s 448us/step - loss: 0.0146 - acc: 1.0000\n",
            "Epoch 67/100\n",
            "192/192 [==============================] - 0s 491us/step - loss: 0.0150 - acc: 1.0000\n",
            "Epoch 68/100\n",
            "192/192 [==============================] - 0s 490us/step - loss: 0.0132 - acc: 1.0000\n",
            "Epoch 69/100\n",
            "192/192 [==============================] - 0s 516us/step - loss: 0.0116 - acc: 1.0000\n",
            "Epoch 70/100\n",
            "192/192 [==============================] - 0s 439us/step - loss: 0.0121 - acc: 1.0000\n",
            "Epoch 71/100\n",
            "192/192 [==============================] - 0s 366us/step - loss: 0.0107 - acc: 1.0000\n",
            "Epoch 72/100\n",
            "192/192 [==============================] - 0s 424us/step - loss: 0.0109 - acc: 1.0000\n",
            "Epoch 73/100\n",
            "192/192 [==============================] - 0s 452us/step - loss: 0.0108 - acc: 1.0000\n",
            "Epoch 74/100\n",
            "192/192 [==============================] - 0s 396us/step - loss: 0.0099 - acc: 1.0000\n",
            "Epoch 75/100\n",
            "192/192 [==============================] - 0s 383us/step - loss: 0.0085 - acc: 1.0000\n",
            "Epoch 76/100\n",
            "192/192 [==============================] - 0s 365us/step - loss: 0.0093 - acc: 1.0000\n",
            "Epoch 77/100\n",
            "192/192 [==============================] - 0s 435us/step - loss: 0.0085 - acc: 1.0000\n",
            "Epoch 78/100\n",
            "192/192 [==============================] - 0s 422us/step - loss: 0.0085 - acc: 1.0000\n",
            "Epoch 79/100\n",
            "192/192 [==============================] - 0s 427us/step - loss: 0.0079 - acc: 1.0000\n",
            "Epoch 80/100\n",
            "192/192 [==============================] - 0s 420us/step - loss: 0.0073 - acc: 1.0000\n",
            "Epoch 81/100\n",
            "192/192 [==============================] - 0s 442us/step - loss: 0.0080 - acc: 1.0000\n",
            "Epoch 82/100\n",
            "192/192 [==============================] - 0s 425us/step - loss: 0.0074 - acc: 1.0000\n",
            "Epoch 83/100\n",
            "192/192 [==============================] - 0s 470us/step - loss: 0.0077 - acc: 1.0000\n",
            "Epoch 84/100\n",
            "192/192 [==============================] - 0s 452us/step - loss: 0.0068 - acc: 1.0000\n",
            "Epoch 85/100\n",
            "192/192 [==============================] - 0s 501us/step - loss: 0.0069 - acc: 1.0000\n",
            "Epoch 86/100\n",
            "192/192 [==============================] - 0s 445us/step - loss: 0.0062 - acc: 1.0000\n",
            "Epoch 87/100\n",
            "192/192 [==============================] - 0s 467us/step - loss: 0.0062 - acc: 1.0000\n",
            "Epoch 88/100\n",
            "192/192 [==============================] - 0s 449us/step - loss: 0.0063 - acc: 1.0000\n",
            "Epoch 89/100\n",
            "192/192 [==============================] - 0s 460us/step - loss: 0.0059 - acc: 1.0000\n",
            "Epoch 90/100\n",
            "192/192 [==============================] - 0s 441us/step - loss: 0.0057 - acc: 1.0000\n",
            "Epoch 91/100\n",
            "192/192 [==============================] - 0s 453us/step - loss: 0.0058 - acc: 1.0000\n",
            "Epoch 92/100\n",
            "192/192 [==============================] - 0s 436us/step - loss: 0.0052 - acc: 1.0000\n",
            "Epoch 93/100\n",
            "192/192 [==============================] - 0s 413us/step - loss: 0.0056 - acc: 1.0000\n",
            "Epoch 94/100\n",
            "192/192 [==============================] - 0s 480us/step - loss: 0.0049 - acc: 1.0000\n",
            "Epoch 95/100\n",
            "192/192 [==============================] - 0s 409us/step - loss: 0.0049 - acc: 1.0000\n",
            "Epoch 96/100\n",
            "192/192 [==============================] - 0s 430us/step - loss: 0.0049 - acc: 1.0000\n",
            "Epoch 97/100\n",
            "192/192 [==============================] - 0s 387us/step - loss: 0.0047 - acc: 1.0000\n",
            "Epoch 98/100\n",
            "192/192 [==============================] - 0s 397us/step - loss: 0.0046 - acc: 1.0000\n",
            "Epoch 99/100\n",
            "192/192 [==============================] - 0s 427us/step - loss: 0.0041 - acc: 1.0000\n",
            "Epoch 100/100\n",
            "192/192 [==============================] - 0s 400us/step - loss: 0.0046 - acc: 1.0000\n",
            "0.7708333333333334 0.7272727272727273 0.9230769230769231 0.5909090909090909 0.5525183631701104\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Model: \"sequential_24\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm1 (LSTM)                 (None, 128)               313344    \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "full_connect (Dense)         (None, 1)                 129       \n",
            "_________________________________________________________________\n",
            "activation_24 (Activation)   (None, 1)                 0         \n",
            "=================================================================\n",
            "Total params: 313,473\n",
            "Trainable params: 313,473\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Compiling the Model...\n",
            "Train...\n",
            "Epoch 1/100\n",
            "192/192 [==============================] - 5s 25ms/step - loss: 0.6894 - acc: 0.5469\n",
            "Epoch 2/100\n",
            "192/192 [==============================] - 0s 443us/step - loss: 0.6681 - acc: 0.7552\n",
            "Epoch 3/100\n",
            "192/192 [==============================] - 0s 486us/step - loss: 0.6507 - acc: 0.7969\n",
            "Epoch 4/100\n",
            "192/192 [==============================] - 0s 484us/step - loss: 0.6347 - acc: 0.8333\n",
            "Epoch 5/100\n",
            "192/192 [==============================] - 0s 378us/step - loss: 0.6107 - acc: 0.8646\n",
            "Epoch 6/100\n",
            "192/192 [==============================] - 0s 379us/step - loss: 0.5905 - acc: 0.8750\n",
            "Epoch 7/100\n",
            "192/192 [==============================] - 0s 417us/step - loss: 0.5698 - acc: 0.8646\n",
            "Epoch 8/100\n",
            "192/192 [==============================] - 0s 423us/step - loss: 0.5462 - acc: 0.8854\n",
            "Epoch 9/100\n",
            "192/192 [==============================] - 0s 442us/step - loss: 0.5198 - acc: 0.8958\n",
            "Epoch 10/100\n",
            "192/192 [==============================] - 0s 362us/step - loss: 0.4938 - acc: 0.9010\n",
            "Epoch 11/100\n",
            "192/192 [==============================] - 0s 349us/step - loss: 0.4640 - acc: 0.9167\n",
            "Epoch 12/100\n",
            "192/192 [==============================] - 0s 442us/step - loss: 0.4372 - acc: 0.9010\n",
            "Epoch 13/100\n",
            "192/192 [==============================] - 0s 467us/step - loss: 0.4073 - acc: 0.9167\n",
            "Epoch 14/100\n",
            "192/192 [==============================] - 0s 462us/step - loss: 0.3769 - acc: 0.9479\n",
            "Epoch 15/100\n",
            "192/192 [==============================] - 0s 472us/step - loss: 0.3508 - acc: 0.9479\n",
            "Epoch 16/100\n",
            "192/192 [==============================] - 0s 502us/step - loss: 0.3284 - acc: 0.9531\n",
            "Epoch 17/100\n",
            "192/192 [==============================] - 0s 446us/step - loss: 0.3033 - acc: 0.9635\n",
            "Epoch 18/100\n",
            "192/192 [==============================] - 0s 421us/step - loss: 0.2797 - acc: 0.9531\n",
            "Epoch 19/100\n",
            "192/192 [==============================] - 0s 419us/step - loss: 0.2586 - acc: 0.9740\n",
            "Epoch 20/100\n",
            "192/192 [==============================] - 0s 384us/step - loss: 0.2406 - acc: 0.9635\n",
            "Epoch 21/100\n",
            "192/192 [==============================] - 0s 451us/step - loss: 0.2203 - acc: 0.9740\n",
            "Epoch 22/100\n",
            "192/192 [==============================] - 0s 465us/step - loss: 0.2055 - acc: 0.9688\n",
            "Epoch 23/100\n",
            "192/192 [==============================] - 0s 476us/step - loss: 0.1902 - acc: 0.9740\n",
            "Epoch 24/100\n",
            "192/192 [==============================] - 0s 472us/step - loss: 0.1769 - acc: 0.9688\n",
            "Epoch 25/100\n",
            "192/192 [==============================] - 0s 501us/step - loss: 0.1662 - acc: 0.9740\n",
            "Epoch 26/100\n",
            "192/192 [==============================] - 0s 460us/step - loss: 0.1540 - acc: 0.9792\n",
            "Epoch 27/100\n",
            "192/192 [==============================] - 0s 469us/step - loss: 0.1430 - acc: 0.9792\n",
            "Epoch 28/100\n",
            "192/192 [==============================] - 0s 389us/step - loss: 0.1331 - acc: 0.9896\n",
            "Epoch 29/100\n",
            "192/192 [==============================] - 0s 420us/step - loss: 0.1265 - acc: 0.9896\n",
            "Epoch 30/100\n",
            "192/192 [==============================] - 0s 425us/step - loss: 0.1169 - acc: 0.9896\n",
            "Epoch 31/100\n",
            "192/192 [==============================] - 0s 375us/step - loss: 0.1094 - acc: 0.9896\n",
            "Epoch 32/100\n",
            "192/192 [==============================] - 0s 366us/step - loss: 0.1037 - acc: 0.9896\n",
            "Epoch 33/100\n",
            "192/192 [==============================] - 0s 428us/step - loss: 0.0965 - acc: 0.9896\n",
            "Epoch 34/100\n",
            "192/192 [==============================] - 0s 433us/step - loss: 0.0890 - acc: 0.9896\n",
            "Epoch 35/100\n",
            "192/192 [==============================] - 0s 368us/step - loss: 0.0838 - acc: 0.9896\n",
            "Epoch 36/100\n",
            "192/192 [==============================] - 0s 425us/step - loss: 0.0807 - acc: 0.9896\n",
            "Epoch 37/100\n",
            "192/192 [==============================] - 0s 395us/step - loss: 0.0742 - acc: 0.9896\n",
            "Epoch 38/100\n",
            "192/192 [==============================] - 0s 423us/step - loss: 0.0689 - acc: 0.9896\n",
            "Epoch 39/100\n",
            "192/192 [==============================] - 0s 377us/step - loss: 0.0666 - acc: 0.9896\n",
            "Epoch 40/100\n",
            "192/192 [==============================] - 0s 369us/step - loss: 0.0618 - acc: 0.9948\n",
            "Epoch 41/100\n",
            "192/192 [==============================] - 0s 439us/step - loss: 0.0581 - acc: 0.9948\n",
            "Epoch 42/100\n",
            "192/192 [==============================] - 0s 441us/step - loss: 0.0570 - acc: 0.9948\n",
            "Epoch 43/100\n",
            "192/192 [==============================] - 0s 391us/step - loss: 0.0525 - acc: 0.9948\n",
            "Epoch 44/100\n",
            "192/192 [==============================] - 0s 376us/step - loss: 0.0498 - acc: 0.9948\n",
            "Epoch 45/100\n",
            "192/192 [==============================] - 0s 376us/step - loss: 0.0487 - acc: 0.9948\n",
            "Epoch 46/100\n",
            "192/192 [==============================] - 0s 424us/step - loss: 0.0454 - acc: 0.9948\n",
            "Epoch 47/100\n",
            "192/192 [==============================] - 0s 435us/step - loss: 0.0436 - acc: 0.9948\n",
            "Epoch 48/100\n",
            "192/192 [==============================] - 0s 384us/step - loss: 0.0394 - acc: 0.9948\n",
            "Epoch 49/100\n",
            "192/192 [==============================] - 0s 416us/step - loss: 0.0371 - acc: 0.9948\n",
            "Epoch 50/100\n",
            "192/192 [==============================] - 0s 424us/step - loss: 0.0369 - acc: 0.9948\n",
            "Epoch 51/100\n",
            "192/192 [==============================] - 0s 442us/step - loss: 0.0361 - acc: 0.9948\n",
            "Epoch 52/100\n",
            "192/192 [==============================] - 0s 433us/step - loss: 0.0333 - acc: 0.9948\n",
            "Epoch 53/100\n",
            "192/192 [==============================] - 0s 439us/step - loss: 0.0300 - acc: 0.9948\n",
            "Epoch 54/100\n",
            "192/192 [==============================] - 0s 394us/step - loss: 0.0297 - acc: 0.9948\n",
            "Epoch 55/100\n",
            "192/192 [==============================] - 0s 424us/step - loss: 0.0281 - acc: 0.9948\n",
            "Epoch 56/100\n",
            "192/192 [==============================] - 0s 428us/step - loss: 0.0268 - acc: 0.9948\n",
            "Epoch 57/100\n",
            "192/192 [==============================] - 0s 460us/step - loss: 0.0248 - acc: 0.9948\n",
            "Epoch 58/100\n",
            "192/192 [==============================] - 0s 554us/step - loss: 0.0237 - acc: 0.9948\n",
            "Epoch 59/100\n",
            "192/192 [==============================] - 0s 481us/step - loss: 0.0233 - acc: 0.9948\n",
            "Epoch 60/100\n",
            "192/192 [==============================] - 0s 491us/step - loss: 0.0222 - acc: 0.9948\n",
            "Epoch 61/100\n",
            "192/192 [==============================] - 0s 395us/step - loss: 0.0202 - acc: 1.0000\n",
            "Epoch 62/100\n",
            "192/192 [==============================] - 0s 409us/step - loss: 0.0208 - acc: 0.9948\n",
            "Epoch 63/100\n",
            "192/192 [==============================] - 0s 435us/step - loss: 0.0199 - acc: 0.9948\n",
            "Epoch 64/100\n",
            "192/192 [==============================] - 0s 397us/step - loss: 0.0180 - acc: 1.0000\n",
            "Epoch 65/100\n",
            "192/192 [==============================] - 0s 422us/step - loss: 0.0186 - acc: 0.9948\n",
            "Epoch 66/100\n",
            "192/192 [==============================] - 0s 413us/step - loss: 0.0152 - acc: 1.0000\n",
            "Epoch 67/100\n",
            "192/192 [==============================] - 0s 390us/step - loss: 0.0158 - acc: 1.0000\n",
            "Epoch 68/100\n",
            "192/192 [==============================] - 0s 478us/step - loss: 0.0147 - acc: 1.0000\n",
            "Epoch 69/100\n",
            "192/192 [==============================] - 0s 483us/step - loss: 0.0139 - acc: 1.0000\n",
            "Epoch 70/100\n",
            "192/192 [==============================] - 0s 473us/step - loss: 0.0128 - acc: 1.0000\n",
            "Epoch 71/100\n",
            "192/192 [==============================] - 0s 455us/step - loss: 0.0141 - acc: 1.0000\n",
            "Epoch 72/100\n",
            "192/192 [==============================] - 0s 412us/step - loss: 0.0128 - acc: 1.0000\n",
            "Epoch 73/100\n",
            "192/192 [==============================] - 0s 440us/step - loss: 0.0119 - acc: 1.0000\n",
            "Epoch 74/100\n",
            "192/192 [==============================] - 0s 443us/step - loss: 0.0112 - acc: 1.0000\n",
            "Epoch 75/100\n",
            "192/192 [==============================] - 0s 441us/step - loss: 0.0107 - acc: 1.0000\n",
            "Epoch 76/100\n",
            "192/192 [==============================] - 0s 445us/step - loss: 0.0116 - acc: 1.0000\n",
            "Epoch 77/100\n",
            "192/192 [==============================] - 0s 463us/step - loss: 0.0096 - acc: 1.0000\n",
            "Epoch 78/100\n",
            "192/192 [==============================] - 0s 424us/step - loss: 0.0093 - acc: 1.0000\n",
            "Epoch 79/100\n",
            "192/192 [==============================] - 0s 503us/step - loss: 0.0091 - acc: 1.0000\n",
            "Epoch 80/100\n",
            "192/192 [==============================] - 0s 450us/step - loss: 0.0087 - acc: 1.0000\n",
            "Epoch 81/100\n",
            "192/192 [==============================] - 0s 464us/step - loss: 0.0081 - acc: 1.0000\n",
            "Epoch 82/100\n",
            "192/192 [==============================] - 0s 471us/step - loss: 0.0080 - acc: 1.0000\n",
            "Epoch 83/100\n",
            "192/192 [==============================] - 0s 435us/step - loss: 0.0084 - acc: 1.0000\n",
            "Epoch 84/100\n",
            "192/192 [==============================] - 0s 478us/step - loss: 0.0078 - acc: 1.0000\n",
            "Epoch 85/100\n",
            "192/192 [==============================] - 0s 459us/step - loss: 0.0071 - acc: 1.0000\n",
            "Epoch 86/100\n",
            "192/192 [==============================] - 0s 448us/step - loss: 0.0071 - acc: 1.0000\n",
            "Epoch 87/100\n",
            "192/192 [==============================] - 0s 459us/step - loss: 0.0075 - acc: 1.0000\n",
            "Epoch 88/100\n",
            "192/192 [==============================] - 0s 479us/step - loss: 0.0067 - acc: 1.0000\n",
            "Epoch 89/100\n",
            "192/192 [==============================] - 0s 475us/step - loss: 0.0070 - acc: 1.0000\n",
            "Epoch 90/100\n",
            "192/192 [==============================] - 0s 464us/step - loss: 0.0065 - acc: 1.0000\n",
            "Epoch 91/100\n",
            "192/192 [==============================] - 0s 392us/step - loss: 0.0063 - acc: 1.0000\n",
            "Epoch 92/100\n",
            "192/192 [==============================] - 0s 450us/step - loss: 0.0056 - acc: 1.0000\n",
            "Epoch 93/100\n",
            "192/192 [==============================] - 0s 396us/step - loss: 0.0056 - acc: 1.0000\n",
            "Epoch 94/100\n",
            "192/192 [==============================] - 0s 382us/step - loss: 0.0050 - acc: 1.0000\n",
            "Epoch 95/100\n",
            "192/192 [==============================] - 0s 382us/step - loss: 0.0053 - acc: 1.0000\n",
            "Epoch 96/100\n",
            "192/192 [==============================] - 0s 384us/step - loss: 0.0054 - acc: 1.0000\n",
            "Epoch 97/100\n",
            "192/192 [==============================] - 0s 437us/step - loss: 0.0051 - acc: 1.0000\n",
            "Epoch 98/100\n",
            "192/192 [==============================] - 0s 446us/step - loss: 0.0046 - acc: 1.0000\n",
            "Epoch 99/100\n",
            "192/192 [==============================] - 0s 398us/step - loss: 0.0044 - acc: 1.0000\n",
            "Epoch 100/100\n",
            "192/192 [==============================] - 0s 417us/step - loss: 0.0047 - acc: 1.0000\n",
            "0.8541666666666666 0.88 0.8461538461538461 0.8636363636363636 0.7079361616697484\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Model: \"sequential_25\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm1 (LSTM)                 (None, 128)               313344    \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "full_connect (Dense)         (None, 1)                 129       \n",
            "_________________________________________________________________\n",
            "activation_25 (Activation)   (None, 1)                 0         \n",
            "=================================================================\n",
            "Total params: 313,473\n",
            "Trainable params: 313,473\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Compiling the Model...\n",
            "Train...\n",
            "Epoch 1/100\n",
            "192/192 [==============================] - 5s 26ms/step - loss: 0.6877 - acc: 0.6198\n",
            "Epoch 2/100\n",
            "192/192 [==============================] - 0s 435us/step - loss: 0.6684 - acc: 0.7344\n",
            "Epoch 3/100\n",
            "192/192 [==============================] - 0s 427us/step - loss: 0.6526 - acc: 0.7812\n",
            "Epoch 4/100\n",
            "192/192 [==============================] - 0s 444us/step - loss: 0.6324 - acc: 0.8333\n",
            "Epoch 5/100\n",
            "192/192 [==============================] - 0s 469us/step - loss: 0.6151 - acc: 0.8385\n",
            "Epoch 6/100\n",
            "192/192 [==============================] - 0s 513us/step - loss: 0.5914 - acc: 0.8698\n",
            "Epoch 7/100\n",
            "192/192 [==============================] - 0s 449us/step - loss: 0.5709 - acc: 0.8802\n",
            "Epoch 8/100\n",
            "192/192 [==============================] - 0s 437us/step - loss: 0.5485 - acc: 0.9062\n",
            "Epoch 9/100\n",
            "192/192 [==============================] - 0s 394us/step - loss: 0.5229 - acc: 0.9167\n",
            "Epoch 10/100\n",
            "192/192 [==============================] - 0s 449us/step - loss: 0.4932 - acc: 0.9219\n",
            "Epoch 11/100\n",
            "192/192 [==============================] - 0s 418us/step - loss: 0.4700 - acc: 0.9219\n",
            "Epoch 12/100\n",
            "192/192 [==============================] - 0s 385us/step - loss: 0.4431 - acc: 0.9271\n",
            "Epoch 13/100\n",
            "192/192 [==============================] - 0s 495us/step - loss: 0.4169 - acc: 0.9271\n",
            "Epoch 14/100\n",
            "192/192 [==============================] - 0s 482us/step - loss: 0.3907 - acc: 0.9219\n",
            "Epoch 15/100\n",
            "192/192 [==============================] - 0s 448us/step - loss: 0.3619 - acc: 0.9323\n",
            "Epoch 16/100\n",
            "192/192 [==============================] - 0s 428us/step - loss: 0.3378 - acc: 0.9323\n",
            "Epoch 17/100\n",
            "192/192 [==============================] - 0s 384us/step - loss: 0.3166 - acc: 0.9375\n",
            "Epoch 18/100\n",
            "192/192 [==============================] - 0s 424us/step - loss: 0.2973 - acc: 0.9323\n",
            "Epoch 19/100\n",
            "192/192 [==============================] - 0s 403us/step - loss: 0.2757 - acc: 0.9323\n",
            "Epoch 20/100\n",
            "192/192 [==============================] - 0s 440us/step - loss: 0.2563 - acc: 0.9323\n",
            "Epoch 21/100\n",
            "192/192 [==============================] - 0s 441us/step - loss: 0.2419 - acc: 0.9375\n",
            "Epoch 22/100\n",
            "192/192 [==============================] - 0s 453us/step - loss: 0.2272 - acc: 0.9375\n",
            "Epoch 23/100\n",
            "192/192 [==============================] - 0s 454us/step - loss: 0.2098 - acc: 0.9531\n",
            "Epoch 24/100\n",
            "192/192 [==============================] - 0s 400us/step - loss: 0.1961 - acc: 0.9479\n",
            "Epoch 25/100\n",
            "192/192 [==============================] - 0s 402us/step - loss: 0.1846 - acc: 0.9583\n",
            "Epoch 26/100\n",
            "192/192 [==============================] - 0s 453us/step - loss: 0.1713 - acc: 0.9479\n",
            "Epoch 27/100\n",
            "192/192 [==============================] - 0s 423us/step - loss: 0.1643 - acc: 0.9583\n",
            "Epoch 28/100\n",
            "192/192 [==============================] - 0s 420us/step - loss: 0.1536 - acc: 0.9688\n",
            "Epoch 29/100\n",
            "192/192 [==============================] - 0s 439us/step - loss: 0.1426 - acc: 0.9740\n",
            "Epoch 30/100\n",
            "192/192 [==============================] - 0s 456us/step - loss: 0.1342 - acc: 0.9740\n",
            "Epoch 31/100\n",
            "192/192 [==============================] - 0s 448us/step - loss: 0.1278 - acc: 0.9740\n",
            "Epoch 32/100\n",
            "192/192 [==============================] - 0s 454us/step - loss: 0.1178 - acc: 0.9792\n",
            "Epoch 33/100\n",
            "192/192 [==============================] - 0s 437us/step - loss: 0.1139 - acc: 0.9792\n",
            "Epoch 34/100\n",
            "192/192 [==============================] - 0s 395us/step - loss: 0.1054 - acc: 0.9792\n",
            "Epoch 35/100\n",
            "192/192 [==============================] - 0s 431us/step - loss: 0.0975 - acc: 0.9844\n",
            "Epoch 36/100\n",
            "192/192 [==============================] - 0s 429us/step - loss: 0.0918 - acc: 0.9844\n",
            "Epoch 37/100\n",
            "192/192 [==============================] - 0s 416us/step - loss: 0.0895 - acc: 0.9844\n",
            "Epoch 38/100\n",
            "192/192 [==============================] - 0s 442us/step - loss: 0.0812 - acc: 0.9844\n",
            "Epoch 39/100\n",
            "192/192 [==============================] - 0s 435us/step - loss: 0.0750 - acc: 0.9844\n",
            "Epoch 40/100\n",
            "192/192 [==============================] - 0s 483us/step - loss: 0.0733 - acc: 0.9844\n",
            "Epoch 41/100\n",
            "192/192 [==============================] - 0s 509us/step - loss: 0.0681 - acc: 0.9896\n",
            "Epoch 42/100\n",
            "192/192 [==============================] - 0s 494us/step - loss: 0.0651 - acc: 0.9844\n",
            "Epoch 43/100\n",
            "192/192 [==============================] - 0s 411us/step - loss: 0.0614 - acc: 0.9896\n",
            "Epoch 44/100\n",
            "192/192 [==============================] - 0s 473us/step - loss: 0.0549 - acc: 0.9948\n",
            "Epoch 45/100\n",
            "192/192 [==============================] - 0s 444us/step - loss: 0.0552 - acc: 0.9948\n",
            "Epoch 46/100\n",
            "192/192 [==============================] - 0s 461us/step - loss: 0.0497 - acc: 0.9948\n",
            "Epoch 47/100\n",
            "192/192 [==============================] - 0s 454us/step - loss: 0.0489 - acc: 0.9896\n",
            "Epoch 48/100\n",
            "192/192 [==============================] - 0s 449us/step - loss: 0.0434 - acc: 0.9896\n",
            "Epoch 49/100\n",
            "192/192 [==============================] - 0s 464us/step - loss: 0.0438 - acc: 0.9896\n",
            "Epoch 50/100\n",
            "192/192 [==============================] - 0s 461us/step - loss: 0.0398 - acc: 0.9948\n",
            "Epoch 51/100\n",
            "192/192 [==============================] - 0s 423us/step - loss: 0.0370 - acc: 0.9948\n",
            "Epoch 52/100\n",
            "192/192 [==============================] - 0s 453us/step - loss: 0.0365 - acc: 0.9948\n",
            "Epoch 53/100\n",
            "192/192 [==============================] - 0s 398us/step - loss: 0.0320 - acc: 1.0000\n",
            "Epoch 54/100\n",
            "192/192 [==============================] - 0s 398us/step - loss: 0.0321 - acc: 1.0000\n",
            "Epoch 55/100\n",
            "192/192 [==============================] - 0s 386us/step - loss: 0.0292 - acc: 1.0000\n",
            "Epoch 56/100\n",
            "192/192 [==============================] - 0s 425us/step - loss: 0.0270 - acc: 1.0000\n",
            "Epoch 57/100\n",
            "192/192 [==============================] - 0s 479us/step - loss: 0.0253 - acc: 1.0000\n",
            "Epoch 58/100\n",
            "192/192 [==============================] - 0s 434us/step - loss: 0.0257 - acc: 1.0000\n",
            "Epoch 59/100\n",
            "192/192 [==============================] - 0s 423us/step - loss: 0.0239 - acc: 1.0000\n",
            "Epoch 60/100\n",
            "192/192 [==============================] - 0s 462us/step - loss: 0.0230 - acc: 1.0000\n",
            "Epoch 61/100\n",
            "192/192 [==============================] - 0s 452us/step - loss: 0.0209 - acc: 1.0000\n",
            "Epoch 62/100\n",
            "192/192 [==============================] - 0s 434us/step - loss: 0.0196 - acc: 1.0000\n",
            "Epoch 63/100\n",
            "192/192 [==============================] - 0s 383us/step - loss: 0.0185 - acc: 1.0000\n",
            "Epoch 64/100\n",
            "192/192 [==============================] - 0s 415us/step - loss: 0.0201 - acc: 1.0000\n",
            "Epoch 65/100\n",
            "192/192 [==============================] - 0s 439us/step - loss: 0.0179 - acc: 1.0000\n",
            "Epoch 66/100\n",
            "192/192 [==============================] - 0s 453us/step - loss: 0.0172 - acc: 1.0000\n",
            "Epoch 67/100\n",
            "192/192 [==============================] - 0s 475us/step - loss: 0.0161 - acc: 1.0000\n",
            "Epoch 68/100\n",
            "192/192 [==============================] - 0s 475us/step - loss: 0.0149 - acc: 1.0000\n",
            "Epoch 69/100\n",
            "192/192 [==============================] - 0s 444us/step - loss: 0.0154 - acc: 1.0000\n",
            "Epoch 70/100\n",
            "192/192 [==============================] - 0s 440us/step - loss: 0.0147 - acc: 1.0000\n",
            "Epoch 71/100\n",
            "192/192 [==============================] - 0s 453us/step - loss: 0.0137 - acc: 1.0000\n",
            "Epoch 72/100\n",
            "192/192 [==============================] - 0s 437us/step - loss: 0.0133 - acc: 1.0000\n",
            "Epoch 73/100\n",
            "192/192 [==============================] - 0s 439us/step - loss: 0.0125 - acc: 1.0000\n",
            "Epoch 74/100\n",
            "192/192 [==============================] - 0s 469us/step - loss: 0.0130 - acc: 1.0000\n",
            "Epoch 75/100\n",
            "192/192 [==============================] - 0s 499us/step - loss: 0.0117 - acc: 1.0000\n",
            "Epoch 76/100\n",
            "192/192 [==============================] - 0s 528us/step - loss: 0.0105 - acc: 1.0000\n",
            "Epoch 77/100\n",
            "192/192 [==============================] - 0s 472us/step - loss: 0.0111 - acc: 1.0000\n",
            "Epoch 78/100\n",
            "192/192 [==============================] - 0s 449us/step - loss: 0.0103 - acc: 1.0000\n",
            "Epoch 79/100\n",
            "192/192 [==============================] - 0s 409us/step - loss: 0.0101 - acc: 1.0000\n",
            "Epoch 80/100\n",
            "192/192 [==============================] - 0s 417us/step - loss: 0.0097 - acc: 1.0000\n",
            "Epoch 81/100\n",
            "192/192 [==============================] - 0s 423us/step - loss: 0.0093 - acc: 1.0000\n",
            "Epoch 82/100\n",
            "192/192 [==============================] - 0s 428us/step - loss: 0.0095 - acc: 1.0000\n",
            "Epoch 83/100\n",
            "192/192 [==============================] - 0s 422us/step - loss: 0.0087 - acc: 1.0000\n",
            "Epoch 84/100\n",
            "192/192 [==============================] - 0s 424us/step - loss: 0.0078 - acc: 1.0000\n",
            "Epoch 85/100\n",
            "192/192 [==============================] - 0s 400us/step - loss: 0.0078 - acc: 1.0000\n",
            "Epoch 86/100\n",
            "192/192 [==============================] - 0s 388us/step - loss: 0.0079 - acc: 1.0000\n",
            "Epoch 87/100\n",
            "192/192 [==============================] - 0s 391us/step - loss: 0.0075 - acc: 1.0000\n",
            "Epoch 88/100\n",
            "192/192 [==============================] - 0s 425us/step - loss: 0.0071 - acc: 1.0000\n",
            "Epoch 89/100\n",
            "192/192 [==============================] - 0s 429us/step - loss: 0.0073 - acc: 1.0000\n",
            "Epoch 90/100\n",
            "192/192 [==============================] - 0s 448us/step - loss: 0.0067 - acc: 1.0000\n",
            "Epoch 91/100\n",
            "192/192 [==============================] - 0s 453us/step - loss: 0.0066 - acc: 1.0000\n",
            "Epoch 92/100\n",
            "192/192 [==============================] - 0s 442us/step - loss: 0.0062 - acc: 1.0000\n",
            "Epoch 93/100\n",
            "192/192 [==============================] - 0s 392us/step - loss: 0.0063 - acc: 1.0000\n",
            "Epoch 94/100\n",
            "192/192 [==============================] - 0s 403us/step - loss: 0.0060 - acc: 1.0000\n",
            "Epoch 95/100\n",
            "192/192 [==============================] - 0s 422us/step - loss: 0.0058 - acc: 1.0000\n",
            "Epoch 96/100\n",
            "192/192 [==============================] - 0s 426us/step - loss: 0.0059 - acc: 1.0000\n",
            "Epoch 97/100\n",
            "192/192 [==============================] - 0s 434us/step - loss: 0.0053 - acc: 1.0000\n",
            "Epoch 98/100\n",
            "192/192 [==============================] - 0s 458us/step - loss: 0.0052 - acc: 1.0000\n",
            "Epoch 99/100\n",
            "192/192 [==============================] - 0s 445us/step - loss: 0.0056 - acc: 1.0000\n",
            "Epoch 100/100\n",
            "192/192 [==============================] - 0s 429us/step - loss: 0.0052 - acc: 1.0000\n",
            "0.875 0.9166666666666666 0.8461538461538461 0.9090909090909091 0.7526178090063816\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Model: \"sequential_26\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm1 (LSTM)                 (None, 128)               313344    \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "full_connect (Dense)         (None, 1)                 129       \n",
            "_________________________________________________________________\n",
            "activation_26 (Activation)   (None, 1)                 0         \n",
            "=================================================================\n",
            "Total params: 313,473\n",
            "Trainable params: 313,473\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Compiling the Model...\n",
            "Train...\n",
            "Epoch 1/100\n",
            "192/192 [==============================] - 5s 28ms/step - loss: 0.6868 - acc: 0.5677\n",
            "Epoch 2/100\n",
            "192/192 [==============================] - 0s 431us/step - loss: 0.6678 - acc: 0.7396\n",
            "Epoch 3/100\n",
            "192/192 [==============================] - 0s 385us/step - loss: 0.6512 - acc: 0.8125\n",
            "Epoch 4/100\n",
            "192/192 [==============================] - 0s 416us/step - loss: 0.6321 - acc: 0.8646\n",
            "Epoch 5/100\n",
            "192/192 [==============================] - 0s 392us/step - loss: 0.6129 - acc: 0.8854\n",
            "Epoch 6/100\n",
            "192/192 [==============================] - 0s 392us/step - loss: 0.5928 - acc: 0.9010\n",
            "Epoch 7/100\n",
            "192/192 [==============================] - 0s 416us/step - loss: 0.5720 - acc: 0.9219\n",
            "Epoch 8/100\n",
            "192/192 [==============================] - 0s 460us/step - loss: 0.5467 - acc: 0.9167\n",
            "Epoch 9/100\n",
            "192/192 [==============================] - 0s 476us/step - loss: 0.5249 - acc: 0.9323\n",
            "Epoch 10/100\n",
            "192/192 [==============================] - 0s 425us/step - loss: 0.4961 - acc: 0.9375\n",
            "Epoch 11/100\n",
            "192/192 [==============================] - 0s 452us/step - loss: 0.4671 - acc: 0.9479\n",
            "Epoch 12/100\n",
            "192/192 [==============================] - 0s 449us/step - loss: 0.4417 - acc: 0.9375\n",
            "Epoch 13/100\n",
            "192/192 [==============================] - 0s 401us/step - loss: 0.4109 - acc: 0.9479\n",
            "Epoch 14/100\n",
            "192/192 [==============================] - 0s 430us/step - loss: 0.3839 - acc: 0.9531\n",
            "Epoch 15/100\n",
            "192/192 [==============================] - 0s 431us/step - loss: 0.3540 - acc: 0.9531\n",
            "Epoch 16/100\n",
            "192/192 [==============================] - 0s 420us/step - loss: 0.3286 - acc: 0.9583\n",
            "Epoch 17/100\n",
            "192/192 [==============================] - 0s 422us/step - loss: 0.3040 - acc: 0.9635\n",
            "Epoch 18/100\n",
            "192/192 [==============================] - 0s 430us/step - loss: 0.2773 - acc: 0.9635\n",
            "Epoch 19/100\n",
            "192/192 [==============================] - 0s 424us/step - loss: 0.2581 - acc: 0.9740\n",
            "Epoch 20/100\n",
            "192/192 [==============================] - 0s 452us/step - loss: 0.2376 - acc: 0.9740\n",
            "Epoch 21/100\n",
            "192/192 [==============================] - 0s 467us/step - loss: 0.2161 - acc: 0.9688\n",
            "Epoch 22/100\n",
            "192/192 [==============================] - 0s 455us/step - loss: 0.1989 - acc: 0.9792\n",
            "Epoch 23/100\n",
            "192/192 [==============================] - 0s 460us/step - loss: 0.1843 - acc: 0.9792\n",
            "Epoch 24/100\n",
            "192/192 [==============================] - 0s 430us/step - loss: 0.1697 - acc: 0.9792\n",
            "Epoch 25/100\n",
            "192/192 [==============================] - 0s 459us/step - loss: 0.1527 - acc: 0.9792\n",
            "Epoch 26/100\n",
            "192/192 [==============================] - 0s 472us/step - loss: 0.1416 - acc: 0.9792\n",
            "Epoch 27/100\n",
            "192/192 [==============================] - 0s 385us/step - loss: 0.1314 - acc: 0.9792\n",
            "Epoch 28/100\n",
            "192/192 [==============================] - 0s 375us/step - loss: 0.1177 - acc: 0.9844\n",
            "Epoch 29/100\n",
            "192/192 [==============================] - 0s 404us/step - loss: 0.1099 - acc: 0.9792\n",
            "Epoch 30/100\n",
            "192/192 [==============================] - 0s 401us/step - loss: 0.1039 - acc: 0.9792\n",
            "Epoch 31/100\n",
            "192/192 [==============================] - 0s 425us/step - loss: 0.0959 - acc: 0.9896\n",
            "Epoch 32/100\n",
            "192/192 [==============================] - 0s 430us/step - loss: 0.0900 - acc: 0.9948\n",
            "Epoch 33/100\n",
            "192/192 [==============================] - 0s 427us/step - loss: 0.0807 - acc: 0.9948\n",
            "Epoch 34/100\n",
            "192/192 [==============================] - 0s 445us/step - loss: 0.0771 - acc: 0.9948\n",
            "Epoch 35/100\n",
            "192/192 [==============================] - 0s 438us/step - loss: 0.0719 - acc: 0.9948\n",
            "Epoch 36/100\n",
            "192/192 [==============================] - 0s 428us/step - loss: 0.0671 - acc: 0.9948\n",
            "Epoch 37/100\n",
            "192/192 [==============================] - 0s 460us/step - loss: 0.0635 - acc: 0.9948\n",
            "Epoch 38/100\n",
            "192/192 [==============================] - 0s 456us/step - loss: 0.0571 - acc: 0.9948\n",
            "Epoch 39/100\n",
            "192/192 [==============================] - 0s 385us/step - loss: 0.0535 - acc: 0.9948\n",
            "Epoch 40/100\n",
            "192/192 [==============================] - 0s 388us/step - loss: 0.0500 - acc: 0.9948\n",
            "Epoch 41/100\n",
            "192/192 [==============================] - 0s 444us/step - loss: 0.0476 - acc: 0.9948\n",
            "Epoch 42/100\n",
            "192/192 [==============================] - 0s 463us/step - loss: 0.0473 - acc: 0.9948\n",
            "Epoch 43/100\n",
            "192/192 [==============================] - 0s 427us/step - loss: 0.0422 - acc: 0.9948\n",
            "Epoch 44/100\n",
            "192/192 [==============================] - 0s 394us/step - loss: 0.0385 - acc: 0.9948\n",
            "Epoch 45/100\n",
            "192/192 [==============================] - 0s 471us/step - loss: 0.0362 - acc: 0.9948\n",
            "Epoch 46/100\n",
            "192/192 [==============================] - 0s 443us/step - loss: 0.0350 - acc: 0.9948\n",
            "Epoch 47/100\n",
            "192/192 [==============================] - 0s 419us/step - loss: 0.0343 - acc: 0.9948\n",
            "Epoch 48/100\n",
            "192/192 [==============================] - 0s 442us/step - loss: 0.0306 - acc: 0.9948\n",
            "Epoch 49/100\n",
            "192/192 [==============================] - 0s 390us/step - loss: 0.0301 - acc: 0.9948\n",
            "Epoch 50/100\n",
            "192/192 [==============================] - 0s 433us/step - loss: 0.0271 - acc: 0.9948\n",
            "Epoch 51/100\n",
            "192/192 [==============================] - 0s 532us/step - loss: 0.0247 - acc: 0.9948\n",
            "Epoch 52/100\n",
            "192/192 [==============================] - 0s 473us/step - loss: 0.0251 - acc: 0.9948\n",
            "Epoch 53/100\n",
            "192/192 [==============================] - 0s 456us/step - loss: 0.0249 - acc: 0.9948\n",
            "Epoch 54/100\n",
            "192/192 [==============================] - 0s 504us/step - loss: 0.0215 - acc: 0.9948\n",
            "Epoch 55/100\n",
            "192/192 [==============================] - 0s 480us/step - loss: 0.0217 - acc: 0.9948\n",
            "Epoch 56/100\n",
            "192/192 [==============================] - 0s 437us/step - loss: 0.0203 - acc: 0.9948\n",
            "Epoch 57/100\n",
            "192/192 [==============================] - 0s 396us/step - loss: 0.0186 - acc: 1.0000\n",
            "Epoch 58/100\n",
            "192/192 [==============================] - 0s 447us/step - loss: 0.0182 - acc: 1.0000\n",
            "Epoch 59/100\n",
            "192/192 [==============================] - 0s 439us/step - loss: 0.0165 - acc: 1.0000\n",
            "Epoch 60/100\n",
            "192/192 [==============================] - 0s 371us/step - loss: 0.0169 - acc: 1.0000\n",
            "Epoch 61/100\n",
            "192/192 [==============================] - 0s 382us/step - loss: 0.0159 - acc: 1.0000\n",
            "Epoch 62/100\n",
            "192/192 [==============================] - 0s 432us/step - loss: 0.0143 - acc: 1.0000\n",
            "Epoch 63/100\n",
            "192/192 [==============================] - 0s 439us/step - loss: 0.0134 - acc: 1.0000\n",
            "Epoch 64/100\n",
            "192/192 [==============================] - 0s 391us/step - loss: 0.0123 - acc: 1.0000\n",
            "Epoch 65/100\n",
            "192/192 [==============================] - 0s 450us/step - loss: 0.0119 - acc: 1.0000\n",
            "Epoch 66/100\n",
            "192/192 [==============================] - 0s 401us/step - loss: 0.0120 - acc: 1.0000\n",
            "Epoch 67/100\n",
            "192/192 [==============================] - 0s 421us/step - loss: 0.0116 - acc: 1.0000\n",
            "Epoch 68/100\n",
            "192/192 [==============================] - 0s 420us/step - loss: 0.0109 - acc: 1.0000\n",
            "Epoch 69/100\n",
            "192/192 [==============================] - 0s 388us/step - loss: 0.0110 - acc: 1.0000\n",
            "Epoch 70/100\n",
            "192/192 [==============================] - 0s 413us/step - loss: 0.0092 - acc: 1.0000\n",
            "Epoch 71/100\n",
            "192/192 [==============================] - 0s 394us/step - loss: 0.0111 - acc: 1.0000\n",
            "Epoch 72/100\n",
            "192/192 [==============================] - 0s 489us/step - loss: 0.0098 - acc: 1.0000\n",
            "Epoch 73/100\n",
            "192/192 [==============================] - 0s 488us/step - loss: 0.0087 - acc: 1.0000\n",
            "Epoch 74/100\n",
            "192/192 [==============================] - 0s 453us/step - loss: 0.0082 - acc: 1.0000\n",
            "Epoch 75/100\n",
            "192/192 [==============================] - 0s 393us/step - loss: 0.0083 - acc: 1.0000\n",
            "Epoch 76/100\n",
            "192/192 [==============================] - 0s 448us/step - loss: 0.0079 - acc: 1.0000\n",
            "Epoch 77/100\n",
            "192/192 [==============================] - 0s 422us/step - loss: 0.0071 - acc: 1.0000\n",
            "Epoch 78/100\n",
            "192/192 [==============================] - 0s 448us/step - loss: 0.0071 - acc: 1.0000\n",
            "Epoch 79/100\n",
            "192/192 [==============================] - 0s 441us/step - loss: 0.0072 - acc: 1.0000\n",
            "Epoch 80/100\n",
            "192/192 [==============================] - 0s 470us/step - loss: 0.0069 - acc: 1.0000\n",
            "Epoch 81/100\n",
            "192/192 [==============================] - 0s 454us/step - loss: 0.0067 - acc: 1.0000\n",
            "Epoch 82/100\n",
            "192/192 [==============================] - 0s 435us/step - loss: 0.0065 - acc: 1.0000\n",
            "Epoch 83/100\n",
            "192/192 [==============================] - 0s 452us/step - loss: 0.0059 - acc: 1.0000\n",
            "Epoch 84/100\n",
            "192/192 [==============================] - 0s 489us/step - loss: 0.0057 - acc: 1.0000\n",
            "Epoch 85/100\n",
            "192/192 [==============================] - 0s 447us/step - loss: 0.0054 - acc: 1.0000\n",
            "Epoch 86/100\n",
            "192/192 [==============================] - 0s 452us/step - loss: 0.0056 - acc: 1.0000\n",
            "Epoch 87/100\n",
            "192/192 [==============================] - 0s 374us/step - loss: 0.0056 - acc: 1.0000\n",
            "Epoch 88/100\n",
            "192/192 [==============================] - 0s 424us/step - loss: 0.0053 - acc: 1.0000\n",
            "Epoch 89/100\n",
            "192/192 [==============================] - 0s 463us/step - loss: 0.0053 - acc: 1.0000\n",
            "Epoch 90/100\n",
            "192/192 [==============================] - 0s 456us/step - loss: 0.0045 - acc: 1.0000\n",
            "Epoch 91/100\n",
            "192/192 [==============================] - 0s 451us/step - loss: 0.0051 - acc: 1.0000\n",
            "Epoch 92/100\n",
            "192/192 [==============================] - 0s 456us/step - loss: 0.0041 - acc: 1.0000\n",
            "Epoch 93/100\n",
            "192/192 [==============================] - 0s 399us/step - loss: 0.0042 - acc: 1.0000\n",
            "Epoch 94/100\n",
            "192/192 [==============================] - 0s 485us/step - loss: 0.0042 - acc: 1.0000\n",
            "Epoch 95/100\n",
            "192/192 [==============================] - 0s 481us/step - loss: 0.0046 - acc: 1.0000\n",
            "Epoch 96/100\n",
            "192/192 [==============================] - 0s 437us/step - loss: 0.0041 - acc: 1.0000\n",
            "Epoch 97/100\n",
            "192/192 [==============================] - 0s 449us/step - loss: 0.0040 - acc: 1.0000\n",
            "Epoch 98/100\n",
            "192/192 [==============================] - 0s 467us/step - loss: 0.0036 - acc: 1.0000\n",
            "Epoch 99/100\n",
            "192/192 [==============================] - 0s 509us/step - loss: 0.0035 - acc: 1.0000\n",
            "Epoch 100/100\n",
            "192/192 [==============================] - 0s 462us/step - loss: 0.0046 - acc: 1.0000\n",
            "0.8125 0.7666666666666667 0.92 0.6956521739130435 0.6352906444286353\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "mean performance of ACP_DL\n",
            "[0.8375     0.83504429 0.884      0.78458498 0.67932295]\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEWCAYAAAB42tAoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nLlmiMMYY49L/B+32X8J33K8bAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbeKrkQoX7ob",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umaUwTk2X7zi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dY1E6AcLX7-S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}