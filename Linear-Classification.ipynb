{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tf2.x-Linear-Classification.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mrzResearchArena/TF-2.X/blob/master/Linear-Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51CB8bMaQG3t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "    %tensorflow_version 2.x  # Colab only.\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "import tensorflow as tf\n",
        "import sklearn\n",
        "import numpy as np\n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYfAz4yvQcMV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import load_breast_cancer \n",
        "D = load_breast_cancer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhKGwF7cRaEH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b3a31865-41a6-4db6-ca43-cccafc06ec55"
      },
      "source": [
        "D.keys()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['data', 'target', 'target_names', 'DESCR', 'feature_names', 'filename'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cc9gTDtsRiUW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249
        },
        "outputId": "f9a1d38f-03e1-4e44-c5eb-6fa11bdeed7c"
      },
      "source": [
        "X = D['data']\n",
        "# X.shape # --> (569, 30)\n",
        "X"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.799e+01, 1.038e+01, 1.228e+02, ..., 2.654e-01, 4.601e-01,\n",
              "        1.189e-01],\n",
              "       [2.057e+01, 1.777e+01, 1.329e+02, ..., 1.860e-01, 2.750e-01,\n",
              "        8.902e-02],\n",
              "       [1.969e+01, 2.125e+01, 1.300e+02, ..., 2.430e-01, 3.613e-01,\n",
              "        8.758e-02],\n",
              "       ...,\n",
              "       [1.660e+01, 2.808e+01, 1.083e+02, ..., 1.418e-01, 2.218e-01,\n",
              "        7.820e-02],\n",
              "       [2.060e+01, 2.933e+01, 1.401e+02, ..., 2.650e-01, 4.087e-01,\n",
              "        1.240e-01],\n",
              "       [7.760e+00, 2.454e+01, 4.792e+01, ..., 0.000e+00, 2.871e-01,\n",
              "        7.039e-02]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Xq9_NzRRofW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 482
        },
        "outputId": "d5bb3572-1837-4aee-b14a-cd1ec7a759ba"
      },
      "source": [
        "Y = D['target']\n",
        "# Y.shape # --> (569,)\n",
        "Y"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
              "       1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,\n",
              "       1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
              "       0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n",
              "       1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,\n",
              "       0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n",
              "       1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n",
              "       1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n",
              "       0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
              "       0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
              "       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n",
              "       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,\n",
              "       1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
              "       1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
              "       1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n",
              "       1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZX_ohg1TBLC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from sklearn.utils import shuffle\n",
        "X, Y = sklearn.utils.shuffle(X, Y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdUls1VhR2l4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d36ced65-6076-46ca-e430-8be419198369"
      },
      "source": [
        "D['target_names'] # malignant --> 0 and benign --> 1"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['malignant', 'benign'], dtype='<U9')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLGXSnbRSQJ5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33, random_state=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jtoMxc-WSrDG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c72317ee-36d0-4619-c543-5f04ed6d0d7f"
      },
      "source": [
        "N, D = X_train.shape\n",
        "print(N, D)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "381 30\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYA6tUGcS7Mo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Input(shape=(D)),\n",
        "    tf.keras.layers.Dense(1, activation='softmax')                             \n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0RWzR1TZM8w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8fda4696-b318-4c2e-f8e4-bb9417ba0863"
      },
      "source": [
        "help(tf.keras.layers.Dense)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Help on class Dense in module tensorflow.python.keras.layers.core:\n",
            "\n",
            "class Dense(tensorflow.python.keras.engine.base_layer.Layer)\n",
            " |  Just your regular densely-connected NN layer.\n",
            " |  \n",
            " |  `Dense` implements the operation:\n",
            " |  `output = activation(dot(input, kernel) + bias)`\n",
            " |  where `activation` is the element-wise activation function\n",
            " |  passed as the `activation` argument, `kernel` is a weights matrix\n",
            " |  created by the layer, and `bias` is a bias vector created by the layer\n",
            " |  (only applicable if `use_bias` is `True`).\n",
            " |  \n",
            " |  Note: If the input to the layer has a rank greater than 2, then\n",
            " |  it is flattened prior to the initial dot product with `kernel`.\n",
            " |  \n",
            " |  Example:\n",
            " |  \n",
            " |  ```python\n",
            " |  # as first layer in a sequential model:\n",
            " |  model = Sequential()\n",
            " |  model.add(Dense(32, input_shape=(16,)))\n",
            " |  # now the model will take as input arrays of shape (*, 16)\n",
            " |  # and output arrays of shape (*, 32)\n",
            " |  \n",
            " |  # after the first layer, you don't need to specify\n",
            " |  # the size of the input anymore:\n",
            " |  model.add(Dense(32))\n",
            " |  ```\n",
            " |  \n",
            " |  Arguments:\n",
            " |    units: Positive integer, dimensionality of the output space.\n",
            " |    activation: Activation function to use.\n",
            " |      If you don't specify anything, no activation is applied\n",
            " |      (ie. \"linear\" activation: `a(x) = x`).\n",
            " |    use_bias: Boolean, whether the layer uses a bias vector.\n",
            " |    kernel_initializer: Initializer for the `kernel` weights matrix.\n",
            " |    bias_initializer: Initializer for the bias vector.\n",
            " |    kernel_regularizer: Regularizer function applied to\n",
            " |      the `kernel` weights matrix.\n",
            " |    bias_regularizer: Regularizer function applied to the bias vector.\n",
            " |    activity_regularizer: Regularizer function applied to\n",
            " |      the output of the layer (its \"activation\")..\n",
            " |    kernel_constraint: Constraint function applied to\n",
            " |      the `kernel` weights matrix.\n",
            " |    bias_constraint: Constraint function applied to the bias vector.\n",
            " |  \n",
            " |  Input shape:\n",
            " |    N-D tensor with shape: `(batch_size, ..., input_dim)`.\n",
            " |    The most common situation would be\n",
            " |    a 2D input with shape `(batch_size, input_dim)`.\n",
            " |  \n",
            " |  Output shape:\n",
            " |    N-D tensor with shape: `(batch_size, ..., units)`.\n",
            " |    For instance, for a 2D input with shape `(batch_size, input_dim)`,\n",
            " |    the output would have shape `(batch_size, units)`.\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      Dense\n",
            " |      tensorflow.python.keras.engine.base_layer.Layer\n",
            " |      tensorflow.python.module.module.Module\n",
            " |      tensorflow.python.training.tracking.tracking.AutoTrackable\n",
            " |      tensorflow.python.training.tracking.base.Trackable\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __init__(self, units, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None, **kwargs)\n",
            " |  \n",
            " |  build(self, input_shape)\n",
            " |      Creates the variables of the layer (optional, for subclass implementers).\n",
            " |      \n",
            " |      This is a method that implementers of subclasses of `Layer` or `Model`\n",
            " |      can override if they need a state-creation step in-between\n",
            " |      layer instantiation and layer call.\n",
            " |      \n",
            " |      This is typically used to create the weights of `Layer` subclasses.\n",
            " |      \n",
            " |      Arguments:\n",
            " |        input_shape: Instance of `TensorShape`, or list of instances of\n",
            " |          `TensorShape` if the layer expects a list of inputs\n",
            " |          (one instance per input).\n",
            " |  \n",
            " |  call(self, inputs)\n",
            " |      This is where the layer's logic lives.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          inputs: Input tensor, or list/tuple of input tensors.\n",
            " |          **kwargs: Additional keyword arguments.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A tensor or list/tuple of tensors.\n",
            " |  \n",
            " |  compute_output_shape(self, input_shape)\n",
            " |      Computes the output shape of the layer.\n",
            " |      \n",
            " |      If the layer has not been built, this method will call `build` on the\n",
            " |      layer. This assumes that the layer will later be used with inputs that\n",
            " |      match the input shape provided here.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          input_shape: Shape tuple (tuple of integers)\n",
            " |              or list of shape tuples (one per output tensor of the layer).\n",
            " |              Shape tuples can include None for free dimensions,\n",
            " |              instead of an integer.\n",
            " |      \n",
            " |      Returns:\n",
            " |          An input shape tuple.\n",
            " |  \n",
            " |  get_config(self)\n",
            " |      Returns the config of the layer.\n",
            " |      \n",
            " |      A layer config is a Python dictionary (serializable)\n",
            " |      containing the configuration of a layer.\n",
            " |      The same layer can be reinstantiated later\n",
            " |      (without its trained weights) from this configuration.\n",
            " |      \n",
            " |      The config of a layer does not include connectivity\n",
            " |      information, nor the layer class name. These are handled\n",
            " |      by `Network` (one layer of abstraction above).\n",
            " |      \n",
            " |      Returns:\n",
            " |          Python dictionary.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from tensorflow.python.keras.engine.base_layer.Layer:\n",
            " |  \n",
            " |  __call__(self, inputs, *args, **kwargs)\n",
            " |      Wraps `call`, applying pre- and post-processing steps.\n",
            " |      \n",
            " |      Arguments:\n",
            " |        inputs: input tensor(s).\n",
            " |        *args: additional positional arguments to be passed to `self.call`.\n",
            " |        **kwargs: additional keyword arguments to be passed to `self.call`.\n",
            " |      \n",
            " |      Returns:\n",
            " |        Output tensor(s).\n",
            " |      \n",
            " |      Note:\n",
            " |        - The following optional keyword arguments are reserved for specific uses:\n",
            " |          * `training`: Boolean scalar tensor of Python boolean indicating\n",
            " |            whether the `call` is meant for training or inference.\n",
            " |          * `mask`: Boolean input mask.\n",
            " |        - If the layer's `call` method takes a `mask` argument (as some Keras\n",
            " |          layers do), its default value will be set to the mask generated\n",
            " |          for `inputs` by the previous layer (if `input` did come from\n",
            " |          a layer that generated a corresponding mask, i.e. if it came from\n",
            " |          a Keras layer with masking support.\n",
            " |      \n",
            " |      Raises:\n",
            " |        ValueError: if the layer's `call` method returns None (an invalid value).\n",
            " |  \n",
            " |  __delattr__(self, name)\n",
            " |      Implement delattr(self, name).\n",
            " |  \n",
            " |  __getstate__(self)\n",
            " |  \n",
            " |  __setattr__(self, name, value)\n",
            " |      Support self.foo = trackable syntax.\n",
            " |  \n",
            " |  __setstate__(self, state)\n",
            " |  \n",
            " |  add_loss(self, losses, inputs=None)\n",
            " |      Add loss tensor(s), potentially dependent on layer inputs.\n",
            " |      \n",
            " |      Some losses (for instance, activity regularization losses) may be dependent\n",
            " |      on the inputs passed when calling a layer. Hence, when reusing the same\n",
            " |      layer on different inputs `a` and `b`, some entries in `layer.losses` may\n",
            " |      be dependent on `a` and some on `b`. This method automatically keeps track\n",
            " |      of dependencies.\n",
            " |      \n",
            " |      This method can be used inside a subclassed layer or model's `call`\n",
            " |      function, in which case `losses` should be a Tensor or list of Tensors.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```python\n",
            " |      class MyLayer(tf.keras.layers.Layer):\n",
            " |        def call(inputs, self):\n",
            " |          self.add_loss(tf.abs(tf.reduce_mean(inputs)), inputs=True)\n",
            " |          return inputs\n",
            " |      ```\n",
            " |      \n",
            " |      This method can also be called directly on a Functional Model during\n",
            " |      construction. In this case, any loss Tensors passed to this Model must\n",
            " |      be symbolic and be able to be traced back to the model's `Input`s. These\n",
            " |      losses become part of the model's topology and are tracked in `get_config`.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```python\n",
            " |      inputs = tf.keras.Input(shape=(10,))\n",
            " |      x = tf.keras.layers.Dense(10)(inputs)\n",
            " |      outputs = tf.keras.layers.Dense(1)(x)\n",
            " |      model = tf.keras.Model(inputs, outputs)\n",
            " |      # Actvity regularization.\n",
            " |      model.add_loss(tf.abs(tf.reduce_mean(x)))\n",
            " |      ```\n",
            " |      \n",
            " |      If this is not the case for your loss (if, for example, your loss references\n",
            " |      a `Variable` of one of the model's layers), you can wrap your loss in a\n",
            " |      zero-argument lambda. These losses are not tracked as part of the model's\n",
            " |      topology since they can't be serialized.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```python\n",
            " |      inputs = tf.keras.Input(shape=(10,))\n",
            " |      x = tf.keras.layers.Dense(10)(inputs)\n",
            " |      outputs = tf.keras.layers.Dense(1)(x)\n",
            " |      model = tf.keras.Model(inputs, outputs)\n",
            " |      # Weight regularization.\n",
            " |      model.add_loss(lambda: tf.reduce_mean(x.kernel))\n",
            " |      ```\n",
            " |      \n",
            " |      The `get_losses_for` method allows to retrieve the losses relevant to a\n",
            " |      specific set of inputs.\n",
            " |      \n",
            " |      Arguments:\n",
            " |        losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses\n",
            " |          may also be zero-argument callables which create a loss tensor.\n",
            " |        inputs: Ignored when executing eagerly. If anything other than None is\n",
            " |          passed, it signals the losses are conditional on some of the layer's\n",
            " |          inputs, and thus they should only be run where these inputs are\n",
            " |          available. This is the case for activity regularization losses, for\n",
            " |          instance. If `None` is passed, the losses are assumed\n",
            " |          to be unconditional, and will apply across all dataflows of the layer\n",
            " |          (e.g. weight regularization losses).\n",
            " |  \n",
            " |  add_metric(self, value, aggregation=None, name=None)\n",
            " |      Adds metric tensor to the layer.\n",
            " |      \n",
            " |      Args:\n",
            " |        value: Metric tensor.\n",
            " |        aggregation: Sample-wise metric reduction function. If `aggregation=None`,\n",
            " |          it indicates that the metric tensor provided has been aggregated\n",
            " |          already. eg, `bin_acc = BinaryAccuracy(name='acc')` followed by\n",
            " |          `model.add_metric(bin_acc(y_true, y_pred))`. If aggregation='mean', the\n",
            " |          given metric tensor will be sample-wise reduced using `mean` function.\n",
            " |          eg, `model.add_metric(tf.reduce_sum(outputs), name='output_mean',\n",
            " |          aggregation='mean')`.\n",
            " |        name: String metric name.\n",
            " |      \n",
            " |      Raises:\n",
            " |        ValueError: If `aggregation` is anything other than None or `mean`.\n",
            " |  \n",
            " |  add_update(self, updates, inputs=None)\n",
            " |      Add update op(s), potentially dependent on layer inputs. (deprecated arguments)\n",
            " |      \n",
            " |      Warning: SOME ARGUMENTS ARE DEPRECATED: `(inputs)`. They will be removed in a future version.\n",
            " |      Instructions for updating:\n",
            " |      `inputs` is now automatically inferred\n",
            " |      \n",
            " |      Weight updates (for instance, the updates of the moving mean and variance\n",
            " |      in a BatchNormalization layer) may be dependent on the inputs passed\n",
            " |      when calling a layer. Hence, when reusing the same layer on\n",
            " |      different inputs `a` and `b`, some entries in `layer.updates` may be\n",
            " |      dependent on `a` and some on `b`. This method automatically keeps track\n",
            " |      of dependencies.\n",
            " |      \n",
            " |      The `get_updates_for` method allows to retrieve the updates relevant to a\n",
            " |      specific set of inputs.\n",
            " |      \n",
            " |      This call is ignored when eager execution is enabled (in that case, variable\n",
            " |      updates are run on the fly and thus do not need to be tracked for later\n",
            " |      execution).\n",
            " |      \n",
            " |      Arguments:\n",
            " |        updates: Update op, or list/tuple of update ops, or zero-arg callable\n",
            " |          that returns an update op. A zero-arg callable should be passed in\n",
            " |          order to disable running the updates by setting `trainable=False`\n",
            " |          on this Layer, when executing in Eager mode.\n",
            " |        inputs: Deprecated, will be automatically inferred.\n",
            " |  \n",
            " |  add_variable(self, *args, **kwargs)\n",
            " |      Deprecated, do NOT use! Alias for `add_weight`. (deprecated)\n",
            " |      \n",
            " |      Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\n",
            " |      Instructions for updating:\n",
            " |      Please use `layer.add_weight` method instead.\n",
            " |  \n",
            " |  add_weight(self, name=None, shape=None, dtype=None, initializer=None, regularizer=None, trainable=None, constraint=None, partitioner=None, use_resource=None, synchronization=<VariableSynchronization.AUTO: 0>, aggregation=<VariableAggregation.NONE: 0>, **kwargs)\n",
            " |      Adds a new variable to the layer.\n",
            " |      \n",
            " |      Arguments:\n",
            " |        name: Variable name.\n",
            " |        shape: Variable shape. Defaults to scalar if unspecified.\n",
            " |        dtype: The type of the variable. Defaults to `self.dtype` or `float32`.\n",
            " |        initializer: Initializer instance (callable).\n",
            " |        regularizer: Regularizer instance (callable).\n",
            " |        trainable: Boolean, whether the variable should be part of the layer's\n",
            " |          \"trainable_variables\" (e.g. variables, biases)\n",
            " |          or \"non_trainable_variables\" (e.g. BatchNorm mean and variance).\n",
            " |          Note that `trainable` cannot be `True` if `synchronization`\n",
            " |          is set to `ON_READ`.\n",
            " |        constraint: Constraint instance (callable).\n",
            " |        partitioner: Partitioner to be passed to the `Trackable` API.\n",
            " |        use_resource: Whether to use `ResourceVariable`.\n",
            " |        synchronization: Indicates when a distributed a variable will be\n",
            " |          aggregated. Accepted values are constants defined in the class\n",
            " |          `tf.VariableSynchronization`. By default the synchronization is set to\n",
            " |          `AUTO` and the current `DistributionStrategy` chooses\n",
            " |          when to synchronize. If `synchronization` is set to `ON_READ`,\n",
            " |          `trainable` must not be set to `True`.\n",
            " |        aggregation: Indicates how a distributed variable will be aggregated.\n",
            " |          Accepted values are constants defined in the class\n",
            " |          `tf.VariableAggregation`.\n",
            " |        **kwargs: Additional keyword arguments. Accepted values are `getter`,\n",
            " |          `collections`, `experimental_autocast` and `caching_device`.\n",
            " |      \n",
            " |      Returns:\n",
            " |        The created variable. Usually either a `Variable` or `ResourceVariable`\n",
            " |        instance. If `partitioner` is not `None`, a `PartitionedVariable`\n",
            " |        instance is returned.\n",
            " |      \n",
            " |      Raises:\n",
            " |        RuntimeError: If called with partitioned variable regularization and\n",
            " |          eager execution is enabled.\n",
            " |        ValueError: When giving unsupported dtype and no initializer or when\n",
            " |          trainable has been set to True with synchronization set as `ON_READ`.\n",
            " |  \n",
            " |  apply(self, inputs, *args, **kwargs)\n",
            " |      Deprecated, do NOT use! (deprecated)\n",
            " |      \n",
            " |      Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\n",
            " |      Instructions for updating:\n",
            " |      Please use `layer.__call__` method instead.\n",
            " |      \n",
            " |      This is an alias of `self.__call__`.\n",
            " |      \n",
            " |      Arguments:\n",
            " |        inputs: Input tensor(s).\n",
            " |        *args: additional positional arguments to be passed to `self.call`.\n",
            " |        **kwargs: additional keyword arguments to be passed to `self.call`.\n",
            " |      \n",
            " |      Returns:\n",
            " |        Output tensor(s).\n",
            " |  \n",
            " |  compute_mask(self, inputs, mask=None)\n",
            " |      Computes an output mask tensor.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          inputs: Tensor or list of tensors.\n",
            " |          mask: Tensor or list of tensors.\n",
            " |      \n",
            " |      Returns:\n",
            " |          None or a tensor (or list of tensors,\n",
            " |              one per output tensor of the layer).\n",
            " |  \n",
            " |  compute_output_signature(self, input_signature)\n",
            " |      Compute the output tensor signature of the layer based on the inputs.\n",
            " |      \n",
            " |      Unlike a TensorShape object, a TensorSpec object contains both shape\n",
            " |      and dtype information for a tensor. This method allows layers to provide\n",
            " |      output dtype information if it is different from the input dtype.\n",
            " |      For any layer that doesn't implement this function,\n",
            " |      the framework will fall back to use `compute_output_shape`, and will\n",
            " |      assume that the output dtype matches the input dtype.\n",
            " |      \n",
            " |      Args:\n",
            " |        input_signature: Single TensorSpec or nested structure of TensorSpec\n",
            " |          objects, describing a candidate input for the layer.\n",
            " |      \n",
            " |      Returns:\n",
            " |        Single TensorSpec or nested structure of TensorSpec objects, describing\n",
            " |          how the layer would transform the provided input.\n",
            " |      \n",
            " |      Raises:\n",
            " |        TypeError: If input_signature contains a non-TensorSpec object.\n",
            " |  \n",
            " |  count_params(self)\n",
            " |      Count the total number of scalars composing the weights.\n",
            " |      \n",
            " |      Returns:\n",
            " |          An integer count.\n",
            " |      \n",
            " |      Raises:\n",
            " |          ValueError: if the layer isn't yet built\n",
            " |            (in which case its weights aren't yet defined).\n",
            " |  \n",
            " |  get_input_at(self, node_index)\n",
            " |      Retrieves the input tensor(s) of a layer at a given node.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          node_index: Integer, index of the node\n",
            " |              from which to retrieve the attribute.\n",
            " |              E.g. `node_index=0` will correspond to the\n",
            " |              first time the layer was called.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A tensor (or list of tensors if the layer has multiple inputs).\n",
            " |      \n",
            " |      Raises:\n",
            " |        RuntimeError: If called in Eager mode.\n",
            " |  \n",
            " |  get_input_mask_at(self, node_index)\n",
            " |      Retrieves the input mask tensor(s) of a layer at a given node.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          node_index: Integer, index of the node\n",
            " |              from which to retrieve the attribute.\n",
            " |              E.g. `node_index=0` will correspond to the\n",
            " |              first time the layer was called.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A mask tensor\n",
            " |          (or list of tensors if the layer has multiple inputs).\n",
            " |  \n",
            " |  get_input_shape_at(self, node_index)\n",
            " |      Retrieves the input shape(s) of a layer at a given node.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          node_index: Integer, index of the node\n",
            " |              from which to retrieve the attribute.\n",
            " |              E.g. `node_index=0` will correspond to the\n",
            " |              first time the layer was called.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A shape tuple\n",
            " |          (or list of shape tuples if the layer has multiple inputs).\n",
            " |      \n",
            " |      Raises:\n",
            " |        RuntimeError: If called in Eager mode.\n",
            " |  \n",
            " |  get_losses_for(self, inputs)\n",
            " |      Retrieves losses relevant to a specific set of inputs.\n",
            " |      \n",
            " |      Arguments:\n",
            " |        inputs: Input tensor or list/tuple of input tensors.\n",
            " |      \n",
            " |      Returns:\n",
            " |        List of loss tensors of the layer that depend on `inputs`.\n",
            " |  \n",
            " |  get_output_at(self, node_index)\n",
            " |      Retrieves the output tensor(s) of a layer at a given node.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          node_index: Integer, index of the node\n",
            " |              from which to retrieve the attribute.\n",
            " |              E.g. `node_index=0` will correspond to the\n",
            " |              first time the layer was called.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A tensor (or list of tensors if the layer has multiple outputs).\n",
            " |      \n",
            " |      Raises:\n",
            " |        RuntimeError: If called in Eager mode.\n",
            " |  \n",
            " |  get_output_mask_at(self, node_index)\n",
            " |      Retrieves the output mask tensor(s) of a layer at a given node.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          node_index: Integer, index of the node\n",
            " |              from which to retrieve the attribute.\n",
            " |              E.g. `node_index=0` will correspond to the\n",
            " |              first time the layer was called.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A mask tensor\n",
            " |          (or list of tensors if the layer has multiple outputs).\n",
            " |  \n",
            " |  get_output_shape_at(self, node_index)\n",
            " |      Retrieves the output shape(s) of a layer at a given node.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          node_index: Integer, index of the node\n",
            " |              from which to retrieve the attribute.\n",
            " |              E.g. `node_index=0` will correspond to the\n",
            " |              first time the layer was called.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A shape tuple\n",
            " |          (or list of shape tuples if the layer has multiple outputs).\n",
            " |      \n",
            " |      Raises:\n",
            " |        RuntimeError: If called in Eager mode.\n",
            " |  \n",
            " |  get_updates_for(self, inputs)\n",
            " |      Retrieves updates relevant to a specific set of inputs.\n",
            " |      \n",
            " |      Arguments:\n",
            " |        inputs: Input tensor or list/tuple of input tensors.\n",
            " |      \n",
            " |      Returns:\n",
            " |        List of update ops of the layer that depend on `inputs`.\n",
            " |  \n",
            " |  get_weights(self)\n",
            " |      Returns the current weights of the layer.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Weights values as a list of numpy arrays.\n",
            " |  \n",
            " |  set_weights(self, weights)\n",
            " |      Sets the weights of the layer, from Numpy arrays.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          weights: a list of Numpy arrays. The number\n",
            " |              of arrays and their shape must match\n",
            " |              number of the dimensions of the weights\n",
            " |              of the layer (i.e. it should match the\n",
            " |              output of `get_weights`).\n",
            " |      \n",
            " |      Raises:\n",
            " |          ValueError: If the provided weights list does not match the\n",
            " |              layer's specifications.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods inherited from tensorflow.python.keras.engine.base_layer.Layer:\n",
            " |  \n",
            " |  from_config(config) from builtins.type\n",
            " |      Creates a layer from its config.\n",
            " |      \n",
            " |      This method is the reverse of `get_config`,\n",
            " |      capable of instantiating the same layer from the config\n",
            " |      dictionary. It does not handle layer connectivity\n",
            " |      (handled by Network), nor weights (handled by `set_weights`).\n",
            " |      \n",
            " |      Arguments:\n",
            " |          config: A Python dictionary, typically the\n",
            " |              output of get_config.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A layer instance.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from tensorflow.python.keras.engine.base_layer.Layer:\n",
            " |  \n",
            " |  activity_regularizer\n",
            " |      Optional regularizer function for the output of this layer.\n",
            " |  \n",
            " |  dtype\n",
            " |  \n",
            " |  dynamic\n",
            " |  \n",
            " |  inbound_nodes\n",
            " |      Deprecated, do NOT use! Only for compatibility with external Keras.\n",
            " |  \n",
            " |  input\n",
            " |      Retrieves the input tensor(s) of a layer.\n",
            " |      \n",
            " |      Only applicable if the layer has exactly one input,\n",
            " |      i.e. if it is connected to one incoming layer.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Input tensor or list of input tensors.\n",
            " |      \n",
            " |      Raises:\n",
            " |        RuntimeError: If called in Eager mode.\n",
            " |        AttributeError: If no inbound nodes are found.\n",
            " |  \n",
            " |  input_mask\n",
            " |      Retrieves the input mask tensor(s) of a layer.\n",
            " |      \n",
            " |      Only applicable if the layer has exactly one inbound node,\n",
            " |      i.e. if it is connected to one incoming layer.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Input mask tensor (potentially None) or list of input\n",
            " |          mask tensors.\n",
            " |      \n",
            " |      Raises:\n",
            " |          AttributeError: if the layer is connected to\n",
            " |          more than one incoming layers.\n",
            " |  \n",
            " |  input_shape\n",
            " |      Retrieves the input shape(s) of a layer.\n",
            " |      \n",
            " |      Only applicable if the layer has exactly one input,\n",
            " |      i.e. if it is connected to one incoming layer, or if all inputs\n",
            " |      have the same shape.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Input shape, as an integer shape tuple\n",
            " |          (or list of shape tuples, one tuple per input tensor).\n",
            " |      \n",
            " |      Raises:\n",
            " |          AttributeError: if the layer has no defined input_shape.\n",
            " |          RuntimeError: if called in Eager mode.\n",
            " |  \n",
            " |  input_spec\n",
            " |  \n",
            " |  losses\n",
            " |      Losses which are associated with this `Layer`.\n",
            " |      \n",
            " |      Variable regularization tensors are created when this property is accessed,\n",
            " |      so it is eager safe: accessing `losses` under a `tf.GradientTape` will\n",
            " |      propagate gradients back to the corresponding variables.\n",
            " |      \n",
            " |      Returns:\n",
            " |        A list of tensors.\n",
            " |  \n",
            " |  metrics\n",
            " |  \n",
            " |  name\n",
            " |      Returns the name of this module as passed or determined in the ctor.\n",
            " |      \n",
            " |      NOTE: This is not the same as the `self.name_scope.name` which includes\n",
            " |      parent module names.\n",
            " |  \n",
            " |  non_trainable_variables\n",
            " |  \n",
            " |  non_trainable_weights\n",
            " |  \n",
            " |  outbound_nodes\n",
            " |      Deprecated, do NOT use! Only for compatibility with external Keras.\n",
            " |  \n",
            " |  output\n",
            " |      Retrieves the output tensor(s) of a layer.\n",
            " |      \n",
            " |      Only applicable if the layer has exactly one output,\n",
            " |      i.e. if it is connected to one incoming layer.\n",
            " |      \n",
            " |      Returns:\n",
            " |        Output tensor or list of output tensors.\n",
            " |      \n",
            " |      Raises:\n",
            " |        AttributeError: if the layer is connected to more than one incoming\n",
            " |          layers.\n",
            " |        RuntimeError: if called in Eager mode.\n",
            " |  \n",
            " |  output_mask\n",
            " |      Retrieves the output mask tensor(s) of a layer.\n",
            " |      \n",
            " |      Only applicable if the layer has exactly one inbound node,\n",
            " |      i.e. if it is connected to one incoming layer.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Output mask tensor (potentially None) or list of output\n",
            " |          mask tensors.\n",
            " |      \n",
            " |      Raises:\n",
            " |          AttributeError: if the layer is connected to\n",
            " |          more than one incoming layers.\n",
            " |  \n",
            " |  output_shape\n",
            " |      Retrieves the output shape(s) of a layer.\n",
            " |      \n",
            " |      Only applicable if the layer has one output,\n",
            " |      or if all outputs have the same shape.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Output shape, as an integer shape tuple\n",
            " |          (or list of shape tuples, one tuple per output tensor).\n",
            " |      \n",
            " |      Raises:\n",
            " |          AttributeError: if the layer has no defined output shape.\n",
            " |          RuntimeError: if called in Eager mode.\n",
            " |  \n",
            " |  stateful\n",
            " |  \n",
            " |  trainable\n",
            " |  \n",
            " |  trainable_variables\n",
            " |      Sequence of trainable variables owned by this module and its submodules.\n",
            " |      \n",
            " |      Note: this method uses reflection to find variables on the current instance\n",
            " |      and submodules. For performance reasons you may wish to cache the result\n",
            " |      of calling this method if you don't expect the return value to change.\n",
            " |      \n",
            " |      Returns:\n",
            " |        A sequence of variables for the current module (sorted by attribute\n",
            " |        name) followed by variables from all submodules recursively (breadth\n",
            " |        first).\n",
            " |  \n",
            " |  trainable_weights\n",
            " |  \n",
            " |  updates\n",
            " |  \n",
            " |  variables\n",
            " |      Returns the list of all layer variables/weights.\n",
            " |      \n",
            " |      Alias of `self.weights`.\n",
            " |      \n",
            " |      Returns:\n",
            " |        A list of variables.\n",
            " |  \n",
            " |  weights\n",
            " |      Returns the list of all layer variables/weights.\n",
            " |      \n",
            " |      Returns:\n",
            " |        A list of variables.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods inherited from tensorflow.python.module.module.Module:\n",
            " |  \n",
            " |  with_name_scope(method) from builtins.type\n",
            " |      Decorator to automatically enter the module name scope.\n",
            " |      \n",
            " |      ```\n",
            " |      class MyModule(tf.Module):\n",
            " |        @tf.Module.with_name_scope\n",
            " |        def __call__(self, x):\n",
            " |          if not hasattr(self, 'w'):\n",
            " |            self.w = tf.Variable(tf.random.normal([x.shape[1], 64]))\n",
            " |          return tf.matmul(x, self.w)\n",
            " |      ```\n",
            " |      \n",
            " |      Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose\n",
            " |      names included the module name:\n",
            " |      \n",
            " |      ```\n",
            " |      mod = MyModule()\n",
            " |      mod(tf.ones([8, 32]))\n",
            " |      # ==> <tf.Tensor: ...>\n",
            " |      mod.w\n",
            " |      # ==> <tf.Variable ...'my_module/w:0'>\n",
            " |      ```\n",
            " |      \n",
            " |      Args:\n",
            " |        method: The method to wrap.\n",
            " |      \n",
            " |      Returns:\n",
            " |        The original method wrapped such that it enters the module's name scope.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from tensorflow.python.module.module.Module:\n",
            " |  \n",
            " |  name_scope\n",
            " |      Returns a `tf.name_scope` instance for this class.\n",
            " |  \n",
            " |  submodules\n",
            " |      Sequence of all sub-modules.\n",
            " |      \n",
            " |      Submodules are modules which are properties of this module, or found as\n",
            " |      properties of modules which are properties of this module (and so on).\n",
            " |      \n",
            " |      ```\n",
            " |      a = tf.Module()\n",
            " |      b = tf.Module()\n",
            " |      c = tf.Module()\n",
            " |      a.b = b\n",
            " |      b.c = c\n",
            " |      assert list(a.submodules) == [b, c]\n",
            " |      assert list(b.submodules) == [c]\n",
            " |      assert list(c.submodules) == []\n",
            " |      ```\n",
            " |      \n",
            " |      Returns:\n",
            " |        A sequence of all submodules.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from tensorflow.python.training.tracking.base.Trackable:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9pVoxcGUS3Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'],\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4_888LkU31C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2cf2e4a8-f18e-486a-af66-327b8191b701"
      },
      "source": [
        "result = model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=100)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 381 samples, validate on 188 samples\n",
            "Epoch 1/100\n",
            "381/381 [==============================] - 0s 1ms/sample - loss: 5.6834 - accuracy: 0.6273 - val_loss: 5.6779 - val_accuracy: 0.6277\n",
            "Epoch 2/100\n",
            "381/381 [==============================] - 0s 97us/sample - loss: 5.6834 - accuracy: 0.6273 - val_loss: 5.6779 - val_accuracy: 0.6277\n",
            "Epoch 3/100\n",
            "381/381 [==============================] - 0s 96us/sample - loss: 5.6834 - accuracy: 0.6273 - val_loss: 5.6779 - val_accuracy: 0.6277\n",
            "Epoch 4/100\n",
            "381/381 [==============================] - 0s 109us/sample - loss: 5.6834 - accuracy: 0.6273 - val_loss: 5.6779 - val_accuracy: 0.6277\n",
            "Epoch 5/100\n",
            "381/381 [==============================] - 0s 108us/sample - loss: 5.6834 - accuracy: 0.6273 - val_loss: 5.6779 - val_accuracy: 0.6277\n",
            "Epoch 6/100\n",
            "381/381 [==============================] - 0s 91us/sample - loss: 5.6834 - accuracy: 0.6273 - val_loss: 5.6779 - val_accuracy: 0.6277\n",
            "Epoch 7/100\n",
            "381/381 [==============================] - 0s 99us/sample - loss: 5.6834 - accuracy: 0.6273 - val_loss: 5.6779 - val_accuracy: 0.6277\n",
            "Epoch 8/100\n",
            "381/381 [==============================] - 0s 95us/sample - loss: 5.6834 - accuracy: 0.6273 - val_loss: 5.6779 - val_accuracy: 0.6277\n",
            "Epoch 9/100\n",
            "381/381 [==============================] - 0s 102us/sample - loss: 5.6834 - accuracy: 0.6273 - val_loss: 5.6779 - val_accuracy: 0.6277\n",
            "Epoch 10/100\n",
            "381/381 [==============================] - 0s 94us/sample - loss: 5.6834 - accuracy: 0.6273 - val_loss: 5.6779 - val_accuracy: 0.6277\n",
            "Epoch 11/100\n",
            "381/381 [==============================] - 0s 98us/sample - loss: 5.6834 - accuracy: 0.6273 - val_loss: 5.6779 - val_accuracy: 0.6277\n",
            "Epoch 12/100\n",
            "381/381 [==============================] - 0s 99us/sample - loss: 5.6834 - accuracy: 0.6273 - val_loss: 5.6779 - val_accuracy: 0.6277\n",
            "Epoch 13/100\n",
            "381/381 [==============================] - 0s 107us/sample - loss: 5.6834 - accuracy: 0.6273 - val_loss: 5.6779 - val_accuracy: 0.6277\n",
            "Epoch 14/100\n",
            "381/381 [==============================] - 0s 103us/sample - loss: 5.6834 - accuracy: 0.6273 - val_loss: 5.6779 - val_accuracy: 0.6277\n",
            "Epoch 15/100\n",
            "381/381 [==============================] - 0s 105us/sample - loss: 5.6834 - accuracy: 0.6273 - val_loss: 5.6779 - val_accuracy: 0.6277\n",
            "Epoch 16/100\n",
            "381/381 [==============================] - 0s 101us/sample - loss: 5.6834 - accuracy: 0.6273 - val_loss: 5.6779 - val_accuracy: 0.6277\n",
            "Epoch 17/100\n",
            "381/381 [==============================] - 0s 93us/sample - loss: 5.6834 - accuracy: 0.6273 - val_loss: 5.6779 - val_accuracy: 0.6277\n",
            "Epoch 18/100\n",
            "381/381 [==============================] - 0s 107us/sample - loss: 5.6834 - accuracy: 0.6273 - val_loss: 5.6779 - val_accuracy: 0.6277\n",
            "Epoch 19/100\n",
            "381/381 [==============================] - 0s 103us/sample - loss: 5.6834 - accuracy: 0.6273 - val_loss: 5.6779 - val_accuracy: 0.6277\n",
            "Epoch 20/100\n",
            "381/381 [==============================] - 0s 107us/sample - loss: 5.6834 - accuracy: 0.6273 - val_loss: 5.6779 - val_accuracy: 0.6277\n",
            "Epoch 21/100\n",
            "381/381 [==============================] - 0s 102us/sample - loss: 5.6834 - accuracy: 0.6273 - val_loss: 5.6779 - val_accuracy: 0.6277\n",
            "Epoch 22/100\n",
            "381/381 [==============================] - 0s 96us/sample - loss: 5.6834 - accuracy: 0.6273 - val_loss: 5.6779 - val_accuracy: 0.6277\n",
            "Epoch 23/100\n",
            "381/381 [==============================] - 0s 104us/sample - loss: 5.6834 - accuracy: 0.6273 - val_loss: 5.6779 - val_accuracy: 0.6277\n",
            "Epoch 24/100\n",
            "381/381 [==============================] - 0s 101us/sample - loss: 5.6834 - accuracy: 0.6273 - val_loss: 5.6779 - val_accuracy: 0.6277\n",
            "Epoch 25/100\n",
            "381/381 [==============================] - 0s 114us/sample - loss: 5.6834 - accuracy: 0.6273 - val_loss: 5.6779 - val_accuracy: 0.6277\n",
            "Epoch 26/100\n",
            "381/381 [==============================] - 0s 108us/sample - loss: 5.6834 - accuracy: 0.6273 - val_loss: 5.6779 - val_accuracy: 0.6277\n",
            "Epoch 27/100\n",
            "381/381 [==============================] - 0s 102us/sample - loss: 5.6834 - accuracy: 0.6273 - val_loss: 5.6779 - val_accuracy: 0.6277\n",
            "Epoch 28/100\n",
            "381/381 [==============================] - 0s 125us/sample - loss: 5.6834 - accuracy: 0.6273 - val_loss: 5.6779 - val_accuracy: 0.6277\n",
            "Epoch 29/100\n",
            "381/381 [==============================] - 0s 116us/sample - loss: 5.6834 - accuracy: 0.6273 - val_loss: 5.6779 - val_accuracy: 0.6277\n",
            "Epoch 30/100\n",
            "381/381 [==============================] - 0s 97us/sample - loss: 5.6834 - accuracy: 0.6273 - val_loss: 5.6779 - val_accuracy: 0.6277\n",
            "Epoch 31/100\n",
            "381/381 [==============================] - 0s 97us/sample - loss: 5.6834 - accuracy: 0.6273 - val_loss: 5.6779 - val_accuracy: 0.6277\n",
            "Epoch 32/100\n",
            "381/381 [==============================] - 0s 92us/sample - loss: 5.6834 - accuracy: 0.6273 - val_loss: 5.6779 - val_accuracy: 0.6277\n",
            "Epoch 33/100\n",
            "381/381 [==============================] - 0s 107us/sample - loss: 5.6834 - accuracy: 0.6273 - val_loss: 5.6779 - val_accuracy: 0.6277\n",
            "Epoch 34/100\n",
            "381/381 [==============================] - 0s 108us/sample - loss: 5.6834 - accuracy: 0.6273 - val_loss: 5.6779 - val_accuracy: 0.6277\n",
            "Epoch 35/100\n",
            "381/381 [==============================] - 0s 102us/sample - loss: 5.6834 - accuracy: 0.6273 - val_loss: 5.6779 - val_accuracy: 0.6277\n",
            "Epoch 36/100\n",
            "381/381 [==============================] - 0s 100us/sample - loss: 5.6834 - accuracy: 0.6273 - val_loss: 5.6779 - val_accuracy: 0.6277\n",
            "Epoch 37/100\n",
            "381/381 [==============================] - 0s 95us/sample - loss: 5.6834 - accuracy: 0.6273 - val_loss: 5.6779 - val_accuracy: 0.6277\n",
            "Epoch 38/100\n",
            "381/381 [==============================] - 0s 96us/sample - loss: 5.6834 - accuracy: 0.6273 - val_loss: 5.6779 - val_accuracy: 0.6277\n",
            "Epoch 39/100\n",
            "381/381 [==============================] - 0s 89us/sample - loss: 5.6834 - accuracy: 0.6273 - val_loss: 5.6779 - val_accuracy: 0.6277\n",
            "Epoch 40/100\n",
            "381/381 [==============================] - 0s 99us/sample - loss: 5.6834 - accuracy: 0.6273 - val_loss: 5.6779 - val_accuracy: 0.6277\n",
            "Epoch 41/100\n",
            "381/381 [==============================] - 0s 94us/sample - loss: 5.6834 - accuracy: 0.6273 - val_loss: 5.6779 - val_accuracy: 0.6277\n",
            "Epoch 42/100\n",
            "381/381 [==============================] - 0s 98us/sample - loss: 5.6834 - accuracy: 0.6273 - val_loss: 5.6779 - val_accuracy: 0.6277\n",
            "Epoch 43/100\n",
            "381/381 [==============================] - 0s 99us/sample - loss: 5.6834 - accuracy: 0.6273 - val_loss: 5.6779 - val_accuracy: 0.6277\n",
            "Epoch 44/100\n",
            "381/381 [==============================] - 0s 105us/sample - loss: 5.6834 - accuracy: 0.6273 - val_loss: 5.6779 - val_accuracy: 0.6277\n",
            "Epoch 45/100\n",
            "381/381 [==============================] - 0s 102us/sample - loss: 5.6834 - accuracy: 0.6273 - val_loss: 5.6779 - val_accuracy: 0.6277\n",
            "Epoch 46/100\n",
            "381/381 [==============================] - 0s 117us/sample - loss: 5.6834 - accuracy: 0.6273 - val_loss: 5.6779 - val_accuracy: 0.6277\n",
            "Epoch 47/100\n",
            "381/381 [==============================] - 0s 97us/sample - loss: 5.6834 - accuracy: 0.6273 - val_loss: 5.6779 - val_accuracy: 0.6277\n",
            "Epoch 48/100\n",
            "381/381 [==============================] - 0s 102us/sample - loss: 5.6834 - accuracy: 0.6273 - val_loss: 5.6779 - val_accuracy: 0.6277\n",
            "Epoch 49/100\n",
            "381/381 [==============================] - 0s 113us/sample - loss: 5.6834 - accuracy: 0.6273 - val_loss: 5.6779 - val_accuracy: 0.6277\n",
            "Epoch 50/100\n",
            "381/381 [==============================] - 0s 119us/sample - loss: 5.6834 - accuracy: 0.6273 - val_loss: 5.6779 - val_accuracy: 0.6277\n",
            "Epoch 51/100\n",
            "381/381 [==============================] - 0s 97us/sample - loss: 5.6834 - accuracy: 0.6273 - val_loss: 5.6779 - val_accuracy: 0.6277\n",
            "Epoch 52/100\n",
            "381/381 [==============================] - 0s 95us/sample - loss: 5.6834 - accuracy: 0.6273 - val_loss: 5.6779 - val_accuracy: 0.6277\n",
            "Epoch 53/100\n",
            "381/381 [==============================] - 0s 91us/sample - loss: 5.6834 - accuracy: 0.6273 - val_loss: 5.6779 - val_accuracy: 0.6277\n",
            "Epoch 54/100\n",
            "381/381 [==============================] - 0s 108us/sample - loss: 5.6834 - accuracy: 0.6273 - val_loss: 5.6779 - val_accuracy: 0.6277\n",
            "Epoch 55/100\n",
            "381/381 [==============================] - 0s 102us/sample - loss: 5.6834 - accuracy: 0.6273 - val_loss: 5.6779 - val_accuracy: 0.6277\n",
            "Epoch 56/100\n",
            "381/381 [==============================] - 0s 108us/sample - loss: 5.6834 - accuracy: 0.6273 - val_loss: 5.6779 - val_accuracy: 0.6277\n",
            "Epoch 57/100\n",
            "381/381 [==============================] - 0s 105us/sample - loss: 5.6834 - accuracy: 0.6273 - val_loss: 5.6779 - val_accuracy: 0.6277\n",
            "Epoch 58/100\n",
            "381/381 [==============================] - 0s 98us/sample - loss: 5.6834 - accuracy: 0.6273 - val_loss: 5.6779 - val_accuracy: 0.6277\n",
            "Epoch 59/100\n",
            "381/381 [==============================] - 0s 98us/sample - loss: 5.6834 - accuracy: 0.6273 - val_loss: 5.6779 - val_accuracy: 0.6277\n",
            "Epoch 60/100\n",
            "381/381 [==============================] - 0s 100us/sample - loss: 5.6834 - accuracy: 0.6273 - val_loss: 5.6779 - val_accuracy: 0.6277\n",
            "Epoch 61/100\n",
            "381/381 [==============================] - 0s 111us/sample - loss: 5.6834 - accuracy: 0.6273 - val_loss: 5.6779 - val_accuracy: 0.6277\n",
            "Epoch 62/100\n",
            "381/381 [==============================] - 0s 103us/sample - loss: 5.6834 - accuracy: 0.6273 - val_loss: 5.6779 - val_accuracy: 0.6277\n",
            "Epoch 63/100\n",
            "381/381 [==============================] - 0s 93us/sample - loss: 5.6834 - accuracy: 0.6273 - val_loss: 5.6779 - val_accuracy: 0.6277\n",
            "Epoch 64/100\n",
            "381/381 [==============================] - 0s 102us/sample - loss: 5.6834 - accuracy: 0.6273 - val_loss: 5.6779 - val_accuracy: 0.6277\n",
            "Epoch 65/100\n",
            "381/381 [==============================] - 0s 93us/sample - loss: 5.6834 - accuracy: 0.6273 - val_loss: 5.6779 - val_accuracy: 0.6277\n",
            "Epoch 66/100\n",
            "381/381 [==============================] - 0s 117us/sample - loss: 5.6834 - accuracy: 0.6273 - val_loss: 5.6779 - val_accuracy: 0.6277\n",
            "Epoch 67/100\n",
            "381/381 [==============================] - 0s 102us/sample - loss: 5.6834 - accuracy: 0.6273 - val_loss: 5.6779 - val_accuracy: 0.6277\n",
            "Epoch 68/100\n",
            "381/381 [==============================] - 0s 104us/sample - loss: 5.6834 - accuracy: 0.6273 - val_loss: 5.6779 - val_accuracy: 0.6277\n",
            "Epoch 69/100\n",
            "381/381 [==============================] - 0s 108us/sample - loss: 5.6834 - accuracy: 0.6273 - val_loss: 5.6779 - val_accuracy: 0.6277\n",
            "Epoch 70/100\n",
            "381/381 [==============================] - 0s 95us/sample - loss: 5.6834 - accuracy: 0.6273 - val_loss: 5.6779 - val_accuracy: 0.6277\n",
            "Epoch 71/100\n",
            "381/381 [==============================] - 0s 100us/sample - loss: 5.6834 - accuracy: 0.6273 - val_loss: 5.6779 - val_accuracy: 0.6277\n",
            "Epoch 72/100\n",
            "381/381 [==============================] - 0s 110us/sample - loss: 5.6834 - accuracy: 0.6273 - val_loss: 5.6779 - val_accuracy: 0.6277\n",
            "Epoch 73/100\n",
            "381/381 [==============================] - 0s 96us/sample - loss: 5.6834 - accuracy: 0.6273 - val_loss: 5.6779 - val_accuracy: 0.6277\n",
            "Epoch 74/100\n",
            "381/381 [==============================] - 0s 95us/sample - loss: 5.6834 - accuracy: 0.6273 - val_loss: 5.6779 - val_accuracy: 0.6277\n",
            "Epoch 75/100\n",
            "381/381 [==============================] - 0s 95us/sample - loss: 5.6834 - accuracy: 0.6273 - val_loss: 5.6779 - val_accuracy: 0.6277\n",
            "Epoch 76/100\n",
            "381/381 [==============================] - 0s 101us/sample - loss: 5.6834 - accuracy: 0.6273 - val_loss: 5.6779 - val_accuracy: 0.6277\n",
            "Epoch 77/100\n",
            "381/381 [==============================] - 0s 100us/sample - loss: 5.6834 - accuracy: 0.6273 - val_loss: 5.6779 - val_accuracy: 0.6277\n",
            "Epoch 78/100\n",
            "381/381 [==============================] - 0s 106us/sample - loss: 5.6834 - accuracy: 0.6273 - val_loss: 5.6779 - val_accuracy: 0.6277\n",
            "Epoch 79/100\n",
            "381/381 [==============================] - 0s 104us/sample - loss: 5.6834 - accuracy: 0.6273 - val_loss: 5.6779 - val_accuracy: 0.6277\n",
            "Epoch 80/100\n",
            "381/381 [==============================] - 0s 95us/sample - loss: 5.6834 - accuracy: 0.6273 - val_loss: 5.6779 - val_accuracy: 0.6277\n",
            "Epoch 81/100\n",
            "381/381 [==============================] - 0s 105us/sample - loss: 5.6834 - accuracy: 0.6273 - val_loss: 5.6779 - val_accuracy: 0.6277\n",
            "Epoch 82/100\n",
            "381/381 [==============================] - 0s 95us/sample - loss: 5.6834 - accuracy: 0.6273 - val_loss: 5.6779 - val_accuracy: 0.6277\n",
            "Epoch 83/100\n",
            "381/381 [==============================] - 0s 95us/sample - loss: 5.6834 - accuracy: 0.6273 - val_loss: 5.6779 - val_accuracy: 0.6277\n",
            "Epoch 84/100\n",
            "381/381 [==============================] - 0s 100us/sample - loss: 5.6834 - accuracy: 0.6273 - val_loss: 5.6779 - val_accuracy: 0.6277\n",
            "Epoch 85/100\n",
            "381/381 [==============================] - 0s 100us/sample - loss: 5.6834 - accuracy: 0.6273 - val_loss: 5.6779 - val_accuracy: 0.6277\n",
            "Epoch 86/100\n",
            "381/381 [==============================] - 0s 97us/sample - loss: 5.6834 - accuracy: 0.6273 - val_loss: 5.6779 - val_accuracy: 0.6277\n",
            "Epoch 87/100\n",
            "381/381 [==============================] - 0s 111us/sample - loss: 5.6834 - accuracy: 0.6273 - val_loss: 5.6779 - val_accuracy: 0.6277\n",
            "Epoch 88/100\n",
            "381/381 [==============================] - 0s 98us/sample - loss: 5.6834 - accuracy: 0.6273 - val_loss: 5.6779 - val_accuracy: 0.6277\n",
            "Epoch 89/100\n",
            "381/381 [==============================] - 0s 102us/sample - loss: 5.6834 - accuracy: 0.6273 - val_loss: 5.6779 - val_accuracy: 0.6277\n",
            "Epoch 90/100\n",
            "381/381 [==============================] - 0s 116us/sample - loss: 5.6834 - accuracy: 0.6273 - val_loss: 5.6779 - val_accuracy: 0.6277\n",
            "Epoch 91/100\n",
            "381/381 [==============================] - 0s 102us/sample - loss: 5.6834 - accuracy: 0.6273 - val_loss: 5.6779 - val_accuracy: 0.6277\n",
            "Epoch 92/100\n",
            "381/381 [==============================] - 0s 96us/sample - loss: 5.6834 - accuracy: 0.6273 - val_loss: 5.6779 - val_accuracy: 0.6277\n",
            "Epoch 93/100\n",
            "381/381 [==============================] - 0s 97us/sample - loss: 5.6834 - accuracy: 0.6273 - val_loss: 5.6779 - val_accuracy: 0.6277\n",
            "Epoch 94/100\n",
            "381/381 [==============================] - 0s 93us/sample - loss: 5.6834 - accuracy: 0.6273 - val_loss: 5.6779 - val_accuracy: 0.6277\n",
            "Epoch 95/100\n",
            "381/381 [==============================] - 0s 110us/sample - loss: 5.6834 - accuracy: 0.6273 - val_loss: 5.6779 - val_accuracy: 0.6277\n",
            "Epoch 96/100\n",
            "381/381 [==============================] - 0s 107us/sample - loss: 5.6834 - accuracy: 0.6273 - val_loss: 5.6779 - val_accuracy: 0.6277\n",
            "Epoch 97/100\n",
            "381/381 [==============================] - 0s 96us/sample - loss: 5.6834 - accuracy: 0.6273 - val_loss: 5.6779 - val_accuracy: 0.6277\n",
            "Epoch 98/100\n",
            "381/381 [==============================] - 0s 105us/sample - loss: 5.6834 - accuracy: 0.6273 - val_loss: 5.6779 - val_accuracy: 0.6277\n",
            "Epoch 99/100\n",
            "381/381 [==============================] - 0s 111us/sample - loss: 5.6834 - accuracy: 0.6273 - val_loss: 5.6779 - val_accuracy: 0.6277\n",
            "Epoch 100/100\n",
            "381/381 [==============================] - 0s 112us/sample - loss: 5.6834 - accuracy: 0.6273 - val_loss: 5.6779 - val_accuracy: 0.6277\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nl7GVx7ZVxa-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "000da599-d866-47d9-ffaf-7766864efc82"
      },
      "source": [
        "print('Training Accuracy: {}'.format(model.evaluate(X_train, Y_train)))\n",
        "print('Testing Accuracy: {}'.format(model.evaluate(X_test, Y_test)))\n",
        "\n",
        "# # Evaluate the model - evaluate() returns loss and accuracy\n",
        "# print(\"Train score:\", model.evaluate(X_train, Y_train))\n",
        "# print(\"Test score:\", model.evaluate(X_test, Y_test))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "381/381 [==============================] - 0s 51us/sample - loss: 5.6834 - accuracy: 0.6273\n",
            "Training Accuracy: [5.683443212133693, 0.62729657]\n",
            "188/188 [==============================] - 0s 68us/sample - loss: 5.6779 - accuracy: 0.6277\n",
            "Testing Accuracy: [5.67790793865285, 0.62765956]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7D0VaZNrXev0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "b53e67a3-deff-487f-aaa7-1fefde85508e"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(result.history['accuracy'], label='loss')\n",
        "plt.plot(result.history['loss'], label='val_loss')\n",
        "plt.legend()"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f3e3b7a1908>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWoAAAD4CAYAAADFAawfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHqSLmhkXrd3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}